{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN, SVM, RF, and KNN for Emotion Classification (DEAP)\n",
    "\n",
    "This notebook compares **shallow and traditional machine learning models** for classifying emotions using the **DEAP dataset**.\n",
    "\n",
    "**Models:** ANN (per-channel with fusion), SVM, RF, KNN (feature fusion)\n",
    "**Targets:** Valence, Arousal\n",
    "**Output:** Final accuracy results only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress all processing messages, progress bars, and warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "# Disable tqdm progress bars\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    import functools\n",
    "    _original_tqdm = tqdm\n",
    "    @functools.wraps(_original_tqdm)\n",
    "    def tqdm(*args, **kwargs):\n",
    "        kwargs['disable'] = True\n",
    "        return _original_tqdm(*args, **kwargs)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Suppress stdout during processing\n",
    "import sys\n",
    "import contextlib\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "metadata_folder_path = '../datasets/DEAP/deap-dataset/Metadata/'\n",
    "channel_data_folder_path = '../datasets/DEAP/deap-dataset/extracted_features'\n",
    "file_path = '../datasets/DEAP/deap-dataset/Metadata/participant_ratings.xls'\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(999)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusable Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN Model\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 64)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.lastlayer = nn.Linear(32, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lastlayer(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def train_ann_per_channel(channel_data_folder_path, train_indices, test_indices, y_train, y_test, device, num_epochs=50, n_folds=5):\n",
    "    \"\"\"Train ANN model per channel and fuse predictions.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    probabilities_list = []\n",
    "    \n",
    "    with suppress_stdout():\n",
    "        for ch_name in os.listdir(channel_data_folder_path):\n",
    "            channel_full_path = os.path.join(channel_data_folder_path, ch_name)\n",
    "            if not os.path.isdir(channel_full_path):\n",
    "                continue\n",
    "            \n",
    "            ch_data = []\n",
    "            for file_name in os.listdir(channel_full_path):\n",
    "                if file_name.endswith('.csv'):\n",
    "                    file_path = os.path.join(channel_full_path, file_name)\n",
    "                    file_data = pd.read_csv(file_path)\n",
    "                    ch_data.append(file_data)\n",
    "            \n",
    "            if not ch_data:\n",
    "                continue\n",
    "            \n",
    "            ch_data = pd.concat(ch_data, axis=0, ignore_index=True)\n",
    "            if 'trial' in ch_data.columns:\n",
    "                ch_data = ch_data.drop(columns=['trial'])\n",
    "            \n",
    "            X = ch_data.values\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "            \n",
    "            X_train = X_scaled[train_indices]\n",
    "            X_test = X_scaled[test_indices]\n",
    "            \n",
    "            X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "            X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "            y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "            \n",
    "            # Cross-validation training\n",
    "            skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "            best_model = None\n",
    "            \n",
    "            for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "                x_tr = X_train_tensor[train_idx].to(device)\n",
    "                y_tr = y_train_tensor[train_idx].to(device)\n",
    "                \n",
    "                train_loader = DataLoader(TensorDataset(x_tr, y_tr), batch_size=64, shuffle=True)\n",
    "                \n",
    "                model = EmotionClassifier(X_train.shape[1], 1).to(device)\n",
    "                optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "                criterion = nn.BCELoss()\n",
    "                \n",
    "                for epoch in range(num_epochs):\n",
    "                    model.train()\n",
    "                    for inputs, labels in train_loader:\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs.squeeze(), labels)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                best_model = model\n",
    "            \n",
    "            # Get probabilities for test set\n",
    "            best_model.eval()\n",
    "            with torch.no_grad():\n",
    "                probabilities = best_model(X_test_tensor).cpu().numpy().squeeze()\n",
    "                probabilities_list.append(probabilities)\n",
    "    \n",
    "    # Fusion: Average probabilities\n",
    "    if probabilities_list:\n",
    "        fused_probabilities = np.mean(probabilities_list, axis=0)\n",
    "        fused_predictions = (fused_probabilities > 0.5).astype(int)\n",
    "        accuracy = accuracy_score(y_test.values, fused_predictions)\n",
    "        return accuracy\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def load_all_channels(channel_data_folder_path, train_indices, test_indices):\n",
    "    \"\"\"Load and concatenate all channel features.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_train_list = []\n",
    "    X_test_list = []\n",
    "    \n",
    "    with suppress_stdout():\n",
    "        for ch_name in os.listdir(channel_data_folder_path):\n",
    "            channel_full_path = os.path.join(channel_data_folder_path, ch_name)\n",
    "            if not os.path.isdir(channel_full_path):\n",
    "                continue\n",
    "            \n",
    "            ch_data = []\n",
    "            for file_name in os.listdir(channel_full_path):\n",
    "                if file_name.endswith('.csv'):\n",
    "                    file_path = os.path.join(channel_full_path, file_name)\n",
    "                    file_data = pd.read_csv(file_path)\n",
    "                    ch_data.append(file_data)\n",
    "            \n",
    "            if not ch_data:\n",
    "                continue\n",
    "            \n",
    "            ch_data = pd.concat(ch_data, axis=0, ignore_index=True)\n",
    "            if 'trial' in ch_data.columns:\n",
    "                ch_data = ch_data.drop(columns=['trial'])\n",
    "            \n",
    "            X = ch_data.values\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "            \n",
    "            X_train_list.append(X_scaled[train_indices])\n",
    "            X_test_list.append(X_scaled[test_indices])\n",
    "    \n",
    "    # Concatenate all channels\n",
    "    if X_train_list:\n",
    "        X_train = np.hstack(X_train_list)\n",
    "        X_test = np.hstack(X_test_list)\n",
    "        return X_train, X_test\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def train_svm(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Train SVM model and return accuracy.\"\"\"\n",
    "    param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "    with suppress_stdout():\n",
    "        grid_search = GridSearchCV(SVC(probability=True, random_state=42), param_grid, cv=5, scoring='accuracy', verbose=0, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        y_pred = grid_search.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "def train_rf(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Train Random Forest model and return accuracy.\"\"\"\n",
    "    param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [10, 20, None]}\n",
    "    with suppress_stdout():\n",
    "        grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy', verbose=0, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        y_pred = grid_search.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "def train_knn(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Train KNN model and return accuracy.\"\"\"\n",
    "    param_grid = {'n_neighbors': [3, 5, 7, 9], 'weights': ['uniform', 'distance']}\n",
    "    with suppress_stdout():\n",
    "        grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy', verbose=0, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        y_pred = grid_search.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load targets\n",
    "targets = pd.read_excel(file_path, index_col=0)\n",
    "valence_targets = targets['Valence']\n",
    "arousal_targets = targets['Arousal']\n",
    "\n",
    "y_valence = (valence_targets >= 4.5).astype(int)\n",
    "y_arousal = (arousal_targets >= 4.5).astype(int)\n",
    "\n",
    "# Split data once for consistency\n",
    "num_samples = len(y_valence)\n",
    "indices = np.arange(num_samples)\n",
    "\n",
    "train_indices_val, test_indices_val = train_test_split(\n",
    "    indices, test_size=0.2, random_state=42, stratify=y_valence\n",
    ")\n",
    "train_indices_aro, test_indices_aro = train_test_split(\n",
    "    indices, test_size=0.2, random_state=42, stratify=y_arousal\n",
    ")\n",
    "\n",
    "y_test_valence = y_valence.iloc[test_indices_val]\n",
    "y_test_arousal = y_arousal.iloc[test_indices_aro]\n",
    "y_train_valence = y_valence.iloc[train_indices_val]\n",
    "y_train_arousal = y_arousal.iloc[train_indices_aro]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Train all models (suppress all output during training)\n",
    "with suppress_stdout():\n",
    "    # ANN: Per-channel with fusion\n",
    "    acc_val_ann = train_ann_per_channel(\n",
    "        channel_data_folder_path, train_indices_val, test_indices_val,\n",
    "        y_train_valence, y_test_valence, device\n",
    "    )\n",
    "    results.append({'Model': 'ANN', 'Target': 'Valence', 'Accuracy': acc_val_ann})\n",
    "\n",
    "    acc_aro_ann = train_ann_per_channel(\n",
    "        channel_data_folder_path, train_indices_aro, test_indices_aro,\n",
    "        y_train_arousal, y_test_arousal, device\n",
    "    )\n",
    "    results.append({'Model': 'ANN', 'Target': 'Arousal', 'Accuracy': acc_aro_ann})\n",
    "\n",
    "    # Load fused features for SVM, RF, KNN\n",
    "    X_train_val, X_test_val = load_all_channels(channel_data_folder_path, train_indices_val, test_indices_val)\n",
    "    X_train_aro, X_test_aro = load_all_channels(channel_data_folder_path, train_indices_aro, test_indices_aro)\n",
    "\n",
    "    if X_train_val is None or X_train_aro is None:\n",
    "        raise ValueError('No channel data found. Please ensure extracted_features directory exists.')\n",
    "\n",
    "    # SVM\n",
    "    acc_val_svm = train_svm(X_train_val, y_train_valence, X_test_val, y_test_valence)\n",
    "    results.append({'Model': 'SVM', 'Target': 'Valence', 'Accuracy': acc_val_svm})\n",
    "\n",
    "    acc_aro_svm = train_svm(X_train_aro, y_train_arousal, X_test_aro, y_test_arousal)\n",
    "    results.append({'Model': 'SVM', 'Target': 'Arousal', 'Accuracy': acc_aro_svm})\n",
    "\n",
    "    # RF\n",
    "    acc_val_rf = train_rf(X_train_val, y_train_valence, X_test_val, y_test_valence)\n",
    "    results.append({'Model': 'RF', 'Target': 'Valence', 'Accuracy': acc_val_rf})\n",
    "\n",
    "    acc_aro_rf = train_rf(X_train_aro, y_train_arousal, X_test_aro, y_test_arousal)\n",
    "    results.append({'Model': 'RF', 'Target': 'Arousal', 'Accuracy': acc_aro_rf})\n",
    "\n",
    "    # KNN\n",
    "    acc_val_knn = train_knn(X_train_val, y_train_valence, X_test_val, y_test_valence)\n",
    "    results.append({'Model': 'KNN', 'Target': 'Valence', 'Accuracy': acc_val_knn})\n",
    "\n",
    "    acc_aro_knn = train_knn(X_train_aro, y_train_arousal, X_test_aro, y_test_arousal)\n",
    "    results.append({'Model': 'KNN', 'Target': 'Arousal', 'Accuracy': acc_aro_knn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results in a clean table\n",
    "results_df = pd.DataFrame(results)\n",
    "results_pivot = results_df.pivot(index='Model', columns='Target', values='Accuracy')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('FINAL ACCURACY RESULTS')\n",
    "print('='*60)\n",
    "print(results_pivot.to_string())\n",
    "print('\\n' + '='*60)\n",
    "print(f'\\nBest Valence: {results_pivot[\"Valence\"].idxmax()} ({results_pivot[\"Valence\"].max():.4f})')\n",
    "print(f'Best Arousal: {results_pivot[\"Arousal\"].idxmax()} ({results_pivot[\"Arousal\"].max():.4f})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
