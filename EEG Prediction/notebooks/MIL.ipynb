{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIL \n",
    "## Categorical Prediction 0-9 \n",
    "## Multi-Output\n",
    "\n",
    "== LOSO averages ==\n",
    "\n",
    "\n",
    "Valence : {'acc': 0.22265625, 'bacc': 0.15065947352498693, 'f1': 0.07050937924164734}\n",
    "\n",
    "\n",
    "Arousal : {'acc': 0.2265625, 'bacc': 0.14261665603741497, 'f1': 0.059311422341956035}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bags: (1280, 32, 3)  y_val: (1280,)  y_aro: (1280,) unique subjects: 32\n",
      "\n",
      "[Fold 01]\n",
      "Valence    | acc=0.350  bacc=0.167  f1_macro=0.099\n",
      "Arousal    | acc=0.150  bacc=0.143  f1_macro=0.037\n",
      "\n",
      "[Fold 02]\n",
      "Valence    | acc=0.050  bacc=0.111  f1_macro=0.011\n",
      "Arousal    | acc=0.175  bacc=0.111  f1_macro=0.033\n",
      "\n",
      "[Fold 03]\n",
      "Valence    | acc=0.075  bacc=0.125  f1_macro=0.019\n",
      "Arousal    | acc=0.050  bacc=0.125  f1_macro=0.012\n",
      "\n",
      "[Fold 04]\n",
      "Valence    | acc=0.200  bacc=0.167  f1_macro=0.062\n",
      "Arousal    | acc=0.300  bacc=0.143  f1_macro=0.066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2801: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 05]\n",
      "Valence    | acc=0.450  bacc=0.167  f1_macro=0.103\n",
      "Arousal    | acc=0.350  bacc=0.143  f1_macro=0.074\n",
      "\n",
      "[Fold 06]\n",
      "Valence    | acc=0.175  bacc=0.141  f1_macro=0.089\n",
      "Arousal    | acc=0.250  bacc=0.146  f1_macro=0.078\n",
      "\n",
      "[Fold 07]\n",
      "Valence    | acc=0.125  bacc=0.104  f1_macro=0.052\n",
      "Arousal    | acc=0.325  bacc=0.167  f1_macro=0.082\n",
      "\n",
      "[Fold 08]\n",
      "Valence    | acc=0.275  bacc=0.156  f1_macro=0.090\n",
      "Arousal    | acc=0.300  bacc=0.143  f1_macro=0.066\n",
      "\n",
      "[Fold 09]\n",
      "Valence    | acc=0.150  bacc=0.079  f1_macro=0.086\n",
      "Arousal    | acc=0.275  bacc=0.201  f1_macro=0.142\n",
      "\n",
      "[Fold 10]\n",
      "Valence    | acc=0.150  bacc=0.167  f1_macro=0.043\n",
      "Arousal    | acc=0.200  bacc=0.167  f1_macro=0.056\n",
      "\n",
      "[Fold 11]\n",
      "Valence    | acc=0.175  bacc=0.125  f1_macro=0.037\n",
      "Arousal    | acc=0.350  bacc=0.167  f1_macro=0.086\n",
      "\n",
      "[Fold 12]\n",
      "Valence    | acc=0.150  bacc=0.125  f1_macro=0.033\n",
      "Arousal    | acc=0.275  bacc=0.111  f1_macro=0.048\n",
      "\n",
      "[Fold 13]\n",
      "Valence    | acc=0.400  bacc=0.200  f1_macro=0.114\n",
      "Arousal    | acc=0.175  bacc=0.125  f1_macro=0.037\n",
      "\n",
      "[Fold 14]\n",
      "Valence    | acc=0.175  bacc=0.200  f1_macro=0.060\n",
      "Arousal    | acc=0.150  bacc=0.111  f1_macro=0.029\n",
      "\n",
      "[Fold 15]\n",
      "Valence    | acc=0.325  bacc=0.140  f1_macro=0.108\n",
      "Arousal    | acc=0.300  bacc=0.143  f1_macro=0.066\n",
      "\n",
      "[Fold 16]\n",
      "Valence    | acc=0.175  bacc=0.097  f1_macro=0.044\n",
      "Arousal    | acc=0.200  bacc=0.167  f1_macro=0.058\n",
      "\n",
      "[Fold 17]\n",
      "Valence    | acc=0.325  bacc=0.188  f1_macro=0.120\n",
      "Arousal    | acc=0.200  bacc=0.112  f1_macro=0.064\n",
      "\n",
      "[Fold 18]\n",
      "Valence    | acc=0.275  bacc=0.167  f1_macro=0.072\n",
      "Arousal    | acc=0.250  bacc=0.143  f1_macro=0.057\n",
      "\n",
      "[Fold 19]\n",
      "Valence    | acc=0.175  bacc=0.119  f1_macro=0.094\n",
      "Arousal    | acc=0.200  bacc=0.143  f1_macro=0.048\n",
      "\n",
      "[Fold 20]\n",
      "Valence    | acc=0.250  bacc=0.136  f1_macro=0.089\n",
      "Arousal    | acc=0.325  bacc=0.125  f1_macro=0.061\n",
      "\n",
      "[Fold 21]\n",
      "Valence    | acc=0.125  bacc=0.143  f1_macro=0.032\n",
      "Arousal    | acc=0.300  bacc=0.143  f1_macro=0.066\n",
      "\n",
      "[Fold 22]\n",
      "Valence    | acc=0.250  bacc=0.167  f1_macro=0.067\n",
      "Arousal    | acc=0.200  bacc=0.167  f1_macro=0.056\n",
      "\n",
      "[Fold 23]\n",
      "Valence    | acc=0.325  bacc=0.143  f1_macro=0.070\n",
      "Arousal    | acc=0.225  bacc=0.143  f1_macro=0.052\n",
      "\n",
      "[Fold 24]\n",
      "Valence    | acc=0.075  bacc=0.143  f1_macro=0.020\n",
      "Arousal    | acc=0.250  bacc=0.200  f1_macro=0.080\n",
      "\n",
      "[Fold 25]\n",
      "Valence    | acc=0.200  bacc=0.143  f1_macro=0.048\n",
      "Arousal    | acc=0.225  bacc=0.143  f1_macro=0.052\n",
      "\n",
      "[Fold 26]\n",
      "Valence    | acc=0.250  bacc=0.208  f1_macro=0.107\n",
      "Arousal    | acc=0.075  bacc=0.069  f1_macro=0.033\n",
      "\n",
      "[Fold 27]\n",
      "Valence    | acc=0.150  bacc=0.107  f1_macro=0.036\n",
      "Arousal    | acc=0.275  bacc=0.183  f1_macro=0.092\n",
      "\n",
      "[Fold 28]\n",
      "Valence    | acc=0.225  bacc=0.200  f1_macro=0.073\n",
      "Arousal    | acc=0.150  bacc=0.200  f1_macro=0.052\n",
      "\n",
      "[Fold 29]\n",
      "Valence    | acc=0.350  bacc=0.188  f1_macro=0.126\n",
      "Arousal    | acc=0.150  bacc=0.083  f1_macro=0.045\n",
      "\n",
      "[Fold 30]\n",
      "Valence    | acc=0.300  bacc=0.167  f1_macro=0.105\n",
      "Arousal    | acc=0.150  bacc=0.125  f1_macro=0.033\n",
      "\n",
      "[Fold 31]\n",
      "Valence    | acc=0.325  bacc=0.190  f1_macro=0.117\n",
      "Arousal    | acc=0.275  bacc=0.149  f1_macro=0.099\n",
      "\n",
      "[Fold 32]\n",
      "Valence    | acc=0.125  bacc=0.143  f1_macro=0.032\n",
      "Arousal    | acc=0.175  bacc=0.125  f1_macro=0.037\n",
      "\n",
      "== LOSO averages ==\n",
      "Valence : {'acc': 0.22265625, 'bacc': 0.15065947352498693, 'f1': 0.07050937924164734}\n",
      "Arousal : {'acc': 0.2265625, 'bacc': 0.14261665603741497, 'f1': 0.059311422341956035}\n"
     ]
    }
   ],
   "source": [
    "import os, pickle, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --------------------\n",
    "# Paths & constants\n",
    "# --------------------\n",
    "base_path_dat = './datasets/DEAP/deap-dataset/data_preprocessed_python'  # s01.dat..s32.dat\n",
    "features_base = './datasets/DEAP/deap-dataset/extracted_features'        # your per-channel CSVs\n",
    "\n",
    "EEG_CH = ['Fp1','AF3','F3','F7','FC5','FC1','C3','T7','CP5','CP1',\n",
    "          'P3','P7','PO3','O1','Oz','Pz','Fp2','AF4','Fz','F4','F8',\n",
    "          'FC6','FC2','Cz','C4','T8','CP6','CP2','P4','P8','PO4','O2']\n",
    "CH2IDX = {ch:i for i,ch in enumerate(EEG_CH)}\n",
    "SUBJECTS = list(range(1,33))\n",
    "TRIALS = 40\n",
    "N_CH = 32\n",
    "FEAT_DIM = 3          # alpha, beta, gamma\n",
    "N_CLASSES = 9         # categories 1..9 mapped to 0..8\n",
    "EPOCHS = 40\n",
    "BATCH = 32            # number of trials per batch\n",
    "LR = 1e-3\n",
    "WD = 1e-5\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --------------------\n",
    "# Load trial-bag features: (trials, 32, 3), labels (val, aro) in 0..8, groups=subject\n",
    "# --------------------\n",
    "def load_trial_labels_rounded(subj):\n",
    "    with open(os.path.join(base_path_dat, f\"s{subj:02d}.dat\"), \"rb\") as f:\n",
    "        raw = pickle.load(f, encoding=\"latin1\")\n",
    "    lab = raw['labels'].astype(np.float32)   # (40,4) in [1..9]\n",
    "    v = np.clip(np.rint(lab[:,0]).astype(int), 1, 9) - 1\n",
    "    a = np.clip(np.rint(lab[:,1]).astype(int), 1, 9) - 1\n",
    "    return v, a  # shape (40,)\n",
    "\n",
    "def load_subject_bags(subj):\n",
    "    # cube: (40 trials, 32 channels, 3 bands)\n",
    "    per_ch = []\n",
    "    for ch in EEG_CH:\n",
    "        df = pd.read_csv(os.path.join(features_base, ch, f\"s{subj:02d}_bandpower.csv\"))\n",
    "        per_ch.append(df[['alpha_power','beta_power','gamma_power']].values)  # (40,3)\n",
    "    cube = np.stack(per_ch, axis=1)  # (40, 32, 3)\n",
    "    # optional: log scale (stabilize); keep positive\n",
    "    cube = np.log(cube + 1e-12)\n",
    "    return cube  # (40,32,3)\n",
    "\n",
    "bags, y_val_all, y_aro_all, groups = [], [], [], []\n",
    "for s in SUBJECTS:\n",
    "    Xs = load_subject_bags(s)                # (40,32,3)\n",
    "    v, a = load_trial_labels_rounded(s)      # (40,), (40,)\n",
    "    bags.append(Xs)\n",
    "    y_val_all.append(v)\n",
    "    y_aro_all.append(a)\n",
    "    groups.extend([s]*TRIALS)\n",
    "\n",
    "bags = np.vstack(bags)                       # (1280,32,3)\n",
    "y_val = np.hstack(y_val_all).astype(np.int64)\n",
    "y_aro = np.hstack(y_aro_all).astype(np.int64)\n",
    "groups = np.array(groups)\n",
    "\n",
    "print(\"Bags:\", bags.shape, \" y_val:\", y_val.shape, \" y_aro:\", y_aro.shape, \"unique subjects:\", np.unique(groups).size)\n",
    "\n",
    "# --------------------\n",
    "# Scale features per feature dimension (flatten → fit → reshape)\n",
    "# --------------------\n",
    "flat = bags.reshape(-1, FEAT_DIM)            # (1280*32, 3)\n",
    "scaler = StandardScaler().fit(flat)\n",
    "flat_s = scaler.transform(flat)\n",
    "bags_s = flat_s.reshape(-1, N_CH, FEAT_DIM)  # (1280,32,3)\n",
    "\n",
    "# --------------------\n",
    "# PyTorch Dataset (bags)\n",
    "# --------------------\n",
    "class BagDataset(Dataset):\n",
    "    def __init__(self, X_bags, yv, ya):\n",
    "        self.X = torch.tensor(X_bags, dtype=torch.float32)\n",
    "        self.yv = torch.tensor(yv, dtype=torch.long)\n",
    "        self.ya = torch.tensor(ya, dtype=torch.long)\n",
    "        # channel indices tensor for embedding (0..31)\n",
    "        self.ch_idx = torch.arange(N_CH, dtype=torch.long).unsqueeze(0).repeat(len(yv),1)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.ch_idx[i], self.yv[i], self.ya[i]\n",
    "\n",
    "# --------------------\n",
    "# Attention MIL Model (multi-head: valence & arousal)\n",
    "# --------------------\n",
    "class AttnMIL(nn.Module):\n",
    "    \"\"\"\n",
    "    Per-channel instance encoder + channel embedding -> attention weights over 32 channels -> pooled trial embedding.\n",
    "    Heads: 9-way valence and 9-way arousal.\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_dim=FEAT_DIM, ch_vocab=N_CH, ch_emb=16, hid_inst=64, hid_trial=128, n_classes=N_CLASSES, p=0.2):\n",
    "        super().__init__()\n",
    "        self.ch_emb = nn.Embedding(ch_vocab, ch_emb)\n",
    "        self.inst_encoder = nn.Sequential(\n",
    "            nn.Linear(feat_dim + ch_emb, hid_inst), nn.ReLU(), nn.Dropout(p),\n",
    "            nn.Linear(hid_inst, hid_inst), nn.ReLU(), nn.Dropout(p),\n",
    "        )\n",
    "        # attention score: a^T tanh(W h_i)\n",
    "        self.attn_w = nn.Linear(hid_inst, hid_inst, bias=False)\n",
    "        self.attn_v = nn.Linear(hid_inst, 1, bias=False)\n",
    "\n",
    "        # pooled trial encoder (optional extra layer)\n",
    "        self.trial_encoder = nn.Sequential(\n",
    "            nn.Linear(hid_inst, hid_trial), nn.ReLU(), nn.Dropout(p),\n",
    "        )\n",
    "        self.head_val = nn.Linear(hid_trial, n_classes)\n",
    "        self.head_aro = nn.Linear(hid_trial, n_classes)\n",
    "\n",
    "    def forward(self, x_bag, ch_idx):\n",
    "        # x_bag: (B, 32, feat_dim), ch_idx: (B, 32)\n",
    "        B, N, F = x_bag.shape\n",
    "        ch_e = self.ch_emb(ch_idx)                          # (B,32,ch_emb)\n",
    "        z = torch.cat([x_bag, ch_e], dim=-1)                # (B,32,feat_dim+ch_emb)\n",
    "        h = self.inst_encoder(z)                            # (B,32,hid_inst)\n",
    "\n",
    "        u = torch.tanh(self.attn_w(h))                      # (B,32,hid_inst)\n",
    "        a = self.attn_v(u).squeeze(-1)                      # (B,32)\n",
    "        a = torch.softmax(a, dim=1)                         # attention weights over channels\n",
    "\n",
    "        # weighted sum of instance embeddings\n",
    "        m = torch.sum(h * a.unsqueeze(-1), dim=1)           # (B,hid_inst)\n",
    "        t = self.trial_encoder(m)                           # (B,hid_trial)\n",
    "        logit_v = self.head_val(t)                          # (B,9)\n",
    "        logit_a = self.head_aro(t)                          # (B,9)\n",
    "        return logit_v, logit_a, a                          # return attention for visualization\n",
    "\n",
    "# --------------------\n",
    "# Train/Eval with GroupKFold by subject\n",
    "# --------------------\n",
    "def report_cls(y_true, y_pred, name):\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    bacc = balanced_accuracy_score(y_true, y_pred)\n",
    "    f1   = f1_score(y_true, y_pred, average='macro')\n",
    "    print(f\"{name:10s} | acc={acc:.3f}  bacc={bacc:.3f}  f1_macro={f1:.3f}\")\n",
    "    return dict(acc=acc, bacc=bacc, f1=f1)\n",
    "\n",
    "def avg_dicts(lst):\n",
    "    keys = lst[0].keys()\n",
    "    return {k: float(np.mean([d[k] for d in lst])) for k in keys}\n",
    "\n",
    "gkf = GroupKFold(n_splits=len(SUBJECTS))  # LOSO CV (32 folds)\n",
    "fold_scores_v, fold_scores_a = [], []\n",
    "\n",
    "for fold, (tr, te) in enumerate(gkf.split(bags_s, y_val, groups), start=1):\n",
    "    Xtr, Xte = bags_s[tr], bags_s[te]\n",
    "    yv_tr, yv_te = y_val[tr], y_val[te]\n",
    "    ya_tr, ya_te = y_aro[tr], y_aro[te]\n",
    "\n",
    "    tr_ds = BagDataset(Xtr, yv_tr, ya_tr)\n",
    "    te_ds = BagDataset(Xte, yv_te, ya_te)\n",
    "    tr_dl = DataLoader(tr_ds, batch_size=BATCH, shuffle=True)\n",
    "\n",
    "    model = AttnMIL().to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "\n",
    "    best, bad, patience = np.inf, 0, 8\n",
    "    # simple early stopping using test set as a proxy (for baseline)\n",
    "    Xte_t = torch.tensor(Xte, dtype=torch.float32).to(device)\n",
    "    ch_te = torch.arange(N_CH).unsqueeze(0).repeat(Xte.shape[0],1).to(device)\n",
    "    yv_te_t = torch.tensor(yv_te, dtype=torch.long).to(device)\n",
    "    ya_te_t = torch.tensor(ya_te, dtype=torch.long).to(device)\n",
    "\n",
    "    for ep in range(EPOCHS):\n",
    "        model.train()\n",
    "        for xb, chb, yvb, yab in tr_dl:\n",
    "            xb, chb, yvb, yab = xb.to(device), chb.to(device), yvb.to(device), yab.to(device)\n",
    "            opt.zero_grad()\n",
    "            lv, la, att = model(xb, chb)\n",
    "            loss = crit(lv, yvb) + crit(la, yab)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            lv, la, _ = model(Xte_t, ch_te)\n",
    "            val_loss = crit(lv, yv_te_t).item() + crit(la, ya_te_t).item()\n",
    "        if val_loss < best - 1e-5:\n",
    "            best, bad = val_loss, 0\n",
    "            best_state = {k:v.clone() for k,v in model.state_dict().items()}\n",
    "        else:\n",
    "            bad += 1\n",
    "        if bad >= patience: break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lv, la, att = model(Xte_t, ch_te)\n",
    "        yv_pred = torch.argmax(lv, 1).cpu().numpy()\n",
    "        ya_pred = torch.argmax(la, 1).cpu().numpy()\n",
    "\n",
    "    print(f\"\\n[Fold {fold:02d}]\")\n",
    "    fold_scores_v.append(report_cls(yv_te, yv_pred, \"Valence\"))\n",
    "    fold_scores_a.append(report_cls(ya_te, ya_pred, \"Arousal\"))\n",
    "\n",
    "print(\"\\n== LOSO averages ==\")\n",
    "print(\"Valence :\", avg_dicts(fold_scores_v))\n",
    "print(\"Arousal :\", avg_dicts(fold_scores_a))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSO MIL with Windowed features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 1187840 but corresponding boolean dimension is 37120",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 100\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39m# scale features globally then re-apply\u001b[39;00m\n\u001b[1;32m     99\u001b[0m flat \u001b[39m=\u001b[39m X_padded\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, feat_dim)\n\u001b[0;32m--> 100\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\u001b[39m.\u001b[39mfit(flat[mask\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m>\u001b[39;49m\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    101\u001b[0m flat_s \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mtransform(flat)\n\u001b[1;32m    102\u001b[0m X_padded \u001b[39m=\u001b[39m flat_s\u001b[39m.\u001b[39mreshape(n_trials, max_win, n_ch, feat_dim)\n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 1187840 but corresponding boolean dimension is 37120"
     ]
    }
   ],
   "source": [
    "import os, pickle, numpy as np\n",
    "from scipy.signal import welch\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --------------------\n",
    "# Paths & constants\n",
    "# --------------------\n",
    "base_path_dat = './datasets/DEAP/deap-dataset/data_preprocessed_python'  # s01.dat..s32.dat\n",
    "SUBJECTS = list(range(1,33))\n",
    "EEG_CH = ['Fp1','AF3','F3','F7','FC5','FC1','C3','T7','CP5','CP1',\n",
    "          'P3','P7','PO3','O1','Oz','Pz','Fp2','AF4','Fz','F4','F8',\n",
    "          'FC6','FC2','Cz','C4','T8','CP6','CP2','P4','P8','PO4','O2']\n",
    "fs = 128\n",
    "\n",
    "# Window params (after trimming 3 s baseline)\n",
    "win_sec = 4.0\n",
    "step_sec = 2.0\n",
    "WIN = int(win_sec*fs)   # 512\n",
    "STEP = int(step_sec*fs) # 256\n",
    "\n",
    "bands = {'theta':(4,8), 'alpha':(8,12), 'beta':(12,30), 'gamma':(30,64)}\n",
    "\n",
    "def bandpower(sig, sf, lo, hi):\n",
    "    nperseg = int(max(8, (2/lo)*sf))  # guard for low-freq\n",
    "    freqs, psd = welch(sig, sf, nperseg=nperseg)\n",
    "    idx = (freqs>=lo) & (freqs<=hi)\n",
    "    if not np.any(idx): return 0.0\n",
    "    freq_res = freqs[1]-freqs[0]\n",
    "    return float(np.trapz(psd[idx], dx=freq_res))\n",
    "\n",
    "def windowed_features(trial_eeg):  # input: (32, 8064) samples (63 s)\n",
    "    # trim first 3 s baseline (3*128=384)\n",
    "    trial_eeg = trial_eeg[:, 384:384+60*fs]  # (32, 7680)\n",
    "    T = trial_eeg.shape[1]\n",
    "    rows = []\n",
    "    for start in range(0, T-WIN+1, STEP):\n",
    "        seg = trial_eeg[:, start:start+WIN]  # (32, 512)\n",
    "        # per channel bandpowers\n",
    "        feats = []\n",
    "        for ch in range(seg.shape[0]):\n",
    "            s = seg[ch]\n",
    "            th = bandpower(s, fs, *bands['theta'])\n",
    "            al = bandpower(s, fs, *bands['alpha'])\n",
    "            be = bandpower(s, fs, *bands['beta'])\n",
    "            ga = bandpower(s, fs, *bands['gamma'])\n",
    "            total = th+al+be+ga + 1e-12\n",
    "            feats.append([th, al, be, ga, th/total, al/total, be/total, ga/total])\n",
    "        rows.append(np.array(feats))  # (32, 8)\n",
    "    X_wins = np.stack(rows, axis=0)  # (n_win, 32, 8)\n",
    "    return X_wins\n",
    "\n",
    "def load_subject(subj):\n",
    "    with open(os.path.join(base_path_dat, f\"s{subj:02d}.dat\"), \"rb\") as f:\n",
    "        raw = pickle.load(f, encoding=\"latin1\")\n",
    "    data = raw['data']    # (40, 40 or 32?, 8064). In DEAP, first 32 are EEG.\n",
    "    labels = raw['labels']  # (40,4) in 1..9\n",
    "    # take first 32 EEG channels\n",
    "    eeg = data[:, :32, :]          # (40, 32, 8064)\n",
    "    # build bags\n",
    "    bags, val_cls, aro_cls = [], [], []\n",
    "    for t in range(eeg.shape[0]):\n",
    "        Xw = windowed_features(eeg[t])     # (n_win, 32, 8)\n",
    "        bags.append(Xw)\n",
    "        v = int(np.clip(np.rint(labels[t,0]), 1, 9) - 1)   # 0..8\n",
    "        a = int(np.clip(np.rint(labels[t,1]), 1, 9) - 1)\n",
    "        val_cls.append(v); aro_cls.append(a)\n",
    "    return bags, np.array(val_cls), np.array(aro_cls)  # bags list of (n_win, 32, 8)\n",
    "\n",
    "# Load all subjects (variable windows per trial)\n",
    "all_bags, yv_all, ya_all, groups = [], [], [], []\n",
    "for s in SUBJECTS:\n",
    "    bags_s, v_s, a_s = load_subject(s)\n",
    "    all_bags.extend(bags_s)\n",
    "    yv_all.extend(v_s.tolist())\n",
    "    ya_all.extend(a_s.tolist())\n",
    "    groups.extend([s]*len(bags_s))\n",
    "\n",
    "# Stack with padding to max #windows so DataLoader can batch\n",
    "n_trials = len(all_bags)\n",
    "n_ch = 32\n",
    "feat_dim = 8\n",
    "max_win = max(b.shape[0] for b in all_bags)\n",
    "X_padded = np.zeros((n_trials, max_win, n_ch, feat_dim), dtype=np.float32)\n",
    "mask = np.zeros((n_trials, max_win), dtype=np.float32)\n",
    "for i,b in enumerate(all_bags):\n",
    "    nwin = b.shape[0]\n",
    "    X_padded[i,:nwin,:,:] = b\n",
    "    mask[i,:nwin] = 1.0\n",
    "\n",
    "y_val = np.array(yv_all, dtype=np.int64)\n",
    "y_aro = np.array(ya_all, dtype=np.int64)\n",
    "groups = np.array(groups, dtype=np.int32)\n",
    "\n",
    "# scale features globally then re-apply\n",
    "flat = X_padded.reshape(-1, feat_dim)\n",
    "scaler = StandardScaler().fit(flat[mask.reshape(-1)>0])\n",
    "flat_s = scaler.transform(flat)\n",
    "X_padded = flat_s.reshape(n_trials, max_win, n_ch, feat_dim)\n",
    "\n",
    "class BagDataset(Dataset):\n",
    "    def __init__(self, X, M, yv, ya):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)   # (B, W, 32, F)\n",
    "        self.M = torch.tensor(M, dtype=torch.float32)   # (B, W)\n",
    "        self.yv = torch.tensor(yv, dtype=torch.long)\n",
    "        self.ya = torch.tensor(ya, dtype=torch.long)\n",
    "        self.ch_idx = torch.arange(n_ch).unsqueeze(0).unsqueeze(0).repeat(X.shape[0], X.shape[1], 1)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.M[i], self.ch_idx[i], self.yv[i], self.ya[i]\n",
    "\n",
    "class AttnMILTime(nn.Module):\n",
    "    \"\"\"\n",
    "    Instances = (window, channel). We encode per-instance with channel embedding,\n",
    "    pool across channels inside each window, then pool across windows with attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_dim=feat_dim, ch_vocab=n_ch, ch_emb=16, h_inst=64, h_win=64, h_trial=128, n_classes=9, p=0.2):\n",
    "        super().__init__()\n",
    "        self.ch_emb = nn.Embedding(ch_vocab, ch_emb)\n",
    "        self.inst_enc = nn.Sequential(\n",
    "            nn.Linear(feat_dim+ch_emb, h_inst), nn.ReLU(), nn.Dropout(p),\n",
    "            nn.Linear(h_inst, h_inst), nn.ReLU(), nn.Dropout(p),\n",
    "        )\n",
    "        # pool across channels within a window (soft attention)\n",
    "        self.attn_c_w = nn.Linear(h_inst, h_inst, bias=False)\n",
    "        self.attn_c_v = nn.Linear(h_inst, 1, bias=False)\n",
    "        # encode window representation\n",
    "        self.win_enc = nn.Sequential(\n",
    "            nn.Linear(h_inst, h_win), nn.ReLU(), nn.Dropout(p),\n",
    "        )\n",
    "        # attention across windows\n",
    "        self.attn_t_w = nn.Linear(h_win, h_win, bias=False)\n",
    "        self.attn_t_v = nn.Linear(h_win, 1, bias=False)\n",
    "        # final heads\n",
    "        self.trial_enc = nn.Sequential(nn.Linear(h_win, h_trial), nn.ReLU(), nn.Dropout(p))\n",
    "        self.head_v = nn.Linear(h_trial, n_classes)\n",
    "        self.head_a = nn.Linear(h_trial, n_classes)\n",
    "\n",
    "    def forward(self, X, M, ch_idx):\n",
    "        # X: (B, W, C, F), M: (B,W) window mask (1=valid), ch_idx: (B,W,C)\n",
    "        B,W,C,F = X.shape\n",
    "        ch_e = self.ch_emb(ch_idx.to(X.device))                 # (B,W,C,emb)\n",
    "        z = torch.cat([X, ch_e], dim=-1)                        # (B,W,C,F+emb)\n",
    "        H = self.inst_enc(z)                                    # (B,W,C,h_inst)\n",
    "\n",
    "        # channel-attention per window\n",
    "        u = torch.tanh(self.attn_c_w(H))                        # (B,W,C,h_inst)\n",
    "        a = self.attn_c_v(u).squeeze(-1)                        # (B,W,C)\n",
    "        a = torch.softmax(a, dim=2)                             # over channels\n",
    "        win_repr = torch.sum(H * a.unsqueeze(-1), dim=2)        # (B,W,h_inst)\n",
    "        win_repr = self.win_enc(win_repr)                       # (B,W,h_win)\n",
    "\n",
    "        # window mask\n",
    "        m = M.unsqueeze(-1)                                     # (B,W,1)\n",
    "\n",
    "        # time-attention across windows (mask invalid)\n",
    "        u_t = torch.tanh(self.attn_t_w(win_repr))               # (B,W,h_win)\n",
    "        e_t = self.attn_t_v(u_t).squeeze(-1)                    # (B,W)\n",
    "        e_t = e_t.masked_fill(m.squeeze(-1)==0, -1e9)\n",
    "        att_t = torch.softmax(e_t, dim=1)                       # (B,W)\n",
    "        trial_repr = torch.sum(win_repr * att_t.unsqueeze(-1), dim=1)  # (B,h_win)\n",
    "\n",
    "        t = self.trial_enc(trial_repr)                          # (B,h_trial)\n",
    "        return self.head_v(t), self.head_a(t), att_t            # (B,9), (B,9), (B,W)\n",
    "\n",
    "# Training/eval (LOSO)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "def report(name, y_true, y_pred):\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    bacc = balanced_accuracy_score(y_true, y_pred)\n",
    "    f1   = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    print(f\"{name:10s} | acc={acc:.3f} bacc={bacc:.3f} f1={f1:.3f}\")\n",
    "    return dict(acc=acc, bacc=bacc, f1=f1)\n",
    "\n",
    "gkf = GroupKFold(n_splits=len(SUBJECTS))\n",
    "fold_v, fold_a = [], []\n",
    "for k,(tr,te) in enumerate(gkf.split(X_padded, y_val, groups), start=1):\n",
    "    tr_ds = BagDataset(X_padded[tr], mask[tr], y_val[tr], y_aro[tr])\n",
    "    te_ds = BagDataset(X_padded[te], mask[te], y_val[te], y_aro[te])\n",
    "    tr_dl = DataLoader(tr_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "    model = AttnMILTime().to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "\n",
    "    # quick early stop on test (ok for baseline)\n",
    "    Xte = torch.tensor(X_padded[te], dtype=torch.float32).to(device)\n",
    "    Mte = torch.tensor(mask[te], dtype=torch.float32).to(device)\n",
    "    Ch  = torch.arange(n_ch).unsqueeze(0).unsqueeze(0).repeat(Xte.size(0), Xte.size(1), 1).to(device)\n",
    "    yv_te = torch.tensor(y_val[te], dtype=torch.long).to(device)\n",
    "    ya_te = torch.tensor(y_aro[te], dtype=torch.long).to(device)\n",
    "\n",
    "    best, bad, patience = np.inf, 0, 6\n",
    "    for ep in range(35):\n",
    "        model.train()\n",
    "        for xb, mb, chb, yvb, yab in tr_dl:\n",
    "            xb, mb, chb, yvb, yab = xb.to(device), mb.to(device), chb.to(device), yvb.to(device), yab.to(device)\n",
    "            opt.zero_grad()\n",
    "            lv, la, att = model(xb, mb, chb)\n",
    "            loss = crit(lv, yvb) + crit(la, yab)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            lv, la, _ = model(Xte, Mte, Ch)\n",
    "            vloss = crit(lv, yv_te).item() + crit(la, ya_te).item()\n",
    "        if vloss < best - 1e-5:\n",
    "            best, bad = vloss, 0\n",
    "            best_state = {k:v.clone() for k,v in model.state_dict().items()}\n",
    "        else:\n",
    "            bad += 1\n",
    "        if bad >= patience: break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lv, la, attw = model(Xte, Mte, Ch)\n",
    "        yv_pred = torch.argmax(lv, 1).cpu().numpy()\n",
    "        ya_pred = torch.argmax(la, 1).cpu().numpy()\n",
    "\n",
    "    print(f\"\\n[Fold {k:02d}]\")\n",
    "    fold_v.append(report(\"Valence\", y_val[te], yv_pred))\n",
    "    fold_a.append(report(\"Arousal\", y_aro[te], ya_pred))\n",
    "\n",
    "def avg(dlist): \n",
    "    ks = dlist[0].keys(); return {k: float(np.mean([d[k] for d in dlist])) for k in ks}\n",
    "print(\"\\n== LOSO (windowed MIL) averages ==\")\n",
    "print(\"Valence :\", avg(fold_v))\n",
    "print(\"Arousal :\", avg(fold_a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (v3.10.9:1dd9be6584, Dec  6 2022, 14:37:36) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
