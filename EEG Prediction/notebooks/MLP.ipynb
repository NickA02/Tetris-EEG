{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Single and Multi Output\n",
    "=== Summary ===\n",
    "\n",
    "\n",
    "               model      acc     bacc       f1\n",
    "\n",
    "\n",
    "      multi_output:valence 0.789062 0.500000 0.441048\n",
    "\n",
    "\n",
    "      multi_output:arousal 0.769531 0.505937 0.450983\n",
    "\n",
    "\n",
    "      single:valence 0.789062 0.500000 0.441048\n",
    "\n",
    "\n",
    "      single:arousal 0.769531 0.505937 0.450983\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------\n",
    "# Paths\n",
    "# --------------------\n",
    "base_path_dat = './datasets/DEAP/deap-dataset/data_preprocessed_python'  # s01.dat ... s32.dat\n",
    "features_base = './datasets/DEAP/deap-dataset/extracted_features'        # per-channel CSVs\n",
    "\n",
    "EEG_CH_NAMES = ['Fp1','AF3','F3','F7','FC5','FC1','C3','T7','CP5','CP1',\n",
    "                'P3','P7','PO3','O1','Oz','Pz','Fp2','AF4','Fz','F4','F8',\n",
    "                'FC6','FC2','Cz','C4','T8','CP6','CP2','P4','P8','PO4','O2']\n",
    "SUBJECTS = range(1, 33)\n",
    "TRIALS_PER_SUBJ = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1280, 96)  y_valence: (1280,)  y_arousal: (1280,)\n",
      "=== Multi-output MLP ===\n",
      "Valence: acc=0.789, bacc=0.500, f1_macro=0.441\n",
      "Arousal: acc=0.770, bacc=0.506, f1_macro=0.451\n",
      "=== Single-output MLP — Valence ===\n",
      "Valence: acc=0.789, bacc=0.500, f1_macro=0.441\n",
      "=== Single-output MLP — Arousal ===\n",
      "Arousal: acc=0.770, bacc=0.506, f1_macro=0.451\n",
      "\n",
      "=== Summary ===\n",
      "               model      acc     bacc       f1\n",
      "multi_output:valence 0.789062 0.500000 0.441048\n",
      "multi_output:arousal 0.769531 0.505937 0.450983\n",
      "      single:valence 0.789062 0.500000 0.441048\n",
      "      single:arousal 0.769531 0.505937 0.450983\n"
     ]
    }
   ],
   "source": [
    "def load_subject_features(subj_id):\n",
    "    \"\"\"Concatenate alpha/beta/gamma for 32 channels -> (40, 96)\"\"\"\n",
    "    per_ch = []\n",
    "    for ch in EEG_CH_NAMES:\n",
    "        csv_path = os.path.join(features_base, ch, f\"s{subj_id:02d}_bandpower.csv\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Expect columns: trial, alpha_power, beta_power, gamma_power\n",
    "        per_ch.append(df[['alpha_power','beta_power','gamma_power']].values)  # (40,3)\n",
    "    X_subj = np.hstack(per_ch)  # (40, 32*3=96)\n",
    "    return X_subj\n",
    "\n",
    "def load_subject_labels(subj_id):\n",
    "    \"\"\"Load per-trial labels from sXX.dat -> (40,4): Valence,Arousal,Dominance,Liking\"\"\"\n",
    "    with open(os.path.join(base_path_dat, f\"s{subj_id:02d}.dat\"), \"rb\") as f:\n",
    "        raw = pickle.load(f, encoding=\"latin1\")\n",
    "    labels = raw['labels'].astype(np.float32)  # (40,4), values 1..9\n",
    "    y_val = (labels[:, 0] > 5).astype(np.int32)  # Valence bin\n",
    "    y_aro = (labels[:, 1] > 5).astype(np.int32)  # Arousal bin\n",
    "    return y_val, y_aro\n",
    "\n",
    "# --------------------\n",
    "# Build X, y with guaranteed alignment\n",
    "# --------------------\n",
    "X_list, yv_list, ya_list = [], [], []\n",
    "for s in SUBJECTS:\n",
    "    Xs = load_subject_features(s)                # (40,96)\n",
    "    yv_s, ya_s = load_subject_labels(s)          # (40,), (40,)\n",
    "    assert Xs.shape[0] == TRIALS_PER_SUBJ == yv_s.shape[0] == ya_s.shape[0]\n",
    "    X_list.append(Xs)\n",
    "    yv_list.append(yv_s)\n",
    "    ya_list.append(ya_s)\n",
    "\n",
    "X = np.vstack(X_list)            # (32*40, 96) = (1280, 96)\n",
    "y_valence = np.hstack(yv_list)   # (1280,)\n",
    "y_arousal = np.hstack(ya_list)   # (1280,)\n",
    "\n",
    "print(\"X shape:\", X.shape, \" y_valence:\", y_valence.shape, \" y_arousal:\", y_arousal.shape)\n",
    "\n",
    "# --------------------\n",
    "# Split + scale\n",
    "# --------------------\n",
    "X_train, X_test, yv_train, yv_test, ya_train, ya_test = train_test_split(\n",
    "    X, y_valence, y_arousal, test_size=0.2, random_state=42, stratify=y_valence\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "yv_train_tensor = torch.tensor(yv_train, dtype=torch.long)\n",
    "yv_test_tensor  = torch.tensor(yv_test,  dtype=torch.long)\n",
    "ya_train_tensor = torch.tensor(ya_train, dtype=torch.long)\n",
    "ya_test_tensor  = torch.tensor(ya_test,  dtype=torch.long)\n",
    "\n",
    "train_ds = TensorDataset(X_train_tensor, yv_train_tensor, ya_train_tensor)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# --------------------\n",
    "# Models\n",
    "# --------------------\n",
    "class MultiOutputMLP(nn.Module):\n",
    "    def __init__(self, input_dim=96, h1=128, h2=64, p=0.3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, h1)\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "        self.do  = nn.Dropout(p)\n",
    "        self.act = nn.ReLU()\n",
    "        self.out_v = nn.Linear(h2, 2)\n",
    "        self.out_a = nn.Linear(h2, 2)\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.do(x)\n",
    "        x = self.act(self.fc2(x))\n",
    "        return self.out_v(x), self.out_a(x)\n",
    "\n",
    "class SingleOutputMLP(nn.Module):\n",
    "    def __init__(self, input_dim=96, h1=128, h2=64, p=0.3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, h1)\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "        self.do  = nn.Dropout(p)\n",
    "        self.act = nn.ReLU()\n",
    "        self.out = nn.Linear(h2, 2)\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.do(x)\n",
    "        x = self.act(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "def report(name, y_true, y_pred):\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    bacc = balanced_accuracy_score(y_true, y_pred)\n",
    "    f1   = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    print(f\"{name}: acc={acc:.3f}, bacc={bacc:.3f}, f1_macro={f1:.3f}\")\n",
    "    return acc, bacc, f1\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EPOCHS = 20\n",
    "\n",
    "# --------------------\n",
    "# Train multi-output\n",
    "# --------------------\n",
    "model_multi = MultiOutputMLP().to(device)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "opt  = optim.Adam(model_multi.parameters(), lr=1e-3)\n",
    "\n",
    "for _ in range(EPOCHS):\n",
    "    model_multi.train()\n",
    "    for xb, yv, ya in train_loader:\n",
    "        xb, yv, ya = xb.to(device), yv.to(device), ya.to(device)\n",
    "        opt.zero_grad()\n",
    "        out_v, out_a = model_multi(xb)\n",
    "        loss = crit(out_v, yv) + crit(out_a, ya)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "model_multi.eval()\n",
    "with torch.no_grad():\n",
    "    out_v, out_a = model_multi(X_test_tensor.to(device))\n",
    "    yv_pred = torch.argmax(out_v, 1).cpu().numpy()\n",
    "    ya_pred = torch.argmax(out_a, 1).cpu().numpy()\n",
    "\n",
    "print(\"=== Multi-output MLP ===\")\n",
    "mv = report(\"Valence\", yv_test, yv_pred)\n",
    "ma = report(\"Arousal\", ya_test, ya_pred)\n",
    "\n",
    "# --------------------\n",
    "# Train & eval single-output models\n",
    "# --------------------\n",
    "def train_single(Xtr, ytr, Xte, yte, label_name):\n",
    "    model = SingleOutputMLP().to(device)\n",
    "    crit  = nn.CrossEntropyLoss()\n",
    "    opt   = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    ds = TensorDataset(torch.tensor(Xtr, dtype=torch.float32),\n",
    "                       torch.tensor(ytr, dtype=torch.long))\n",
    "    dl = DataLoader(ds, batch_size=64, shuffle=True)\n",
    "    for _ in range(EPOCHS):\n",
    "        model.train()\n",
    "        for xb, yb in dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.tensor(Xte, dtype=torch.float32).to(device))\n",
    "        y_pred = torch.argmax(logits, 1).cpu().numpy()\n",
    "    print(f\"=== Single-output MLP — {label_name} ===\")\n",
    "    return report(label_name, yte, y_pred)\n",
    "\n",
    "sv = train_single(X_train, yv_train, X_test, yv_test, \"Valence\")\n",
    "sa = train_single(X_train, ya_train, X_test, ya_test, \"Arousal\")\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"model\": [\"multi_output:valence\",\"multi_output:arousal\",\"single:valence\",\"single:arousal\"],\n",
    "    \"acc\":   [mv[0], ma[0], sv[0], sa[0]],\n",
    "    \"bacc\":  [mv[1], ma[1], sv[1], sa[1]],\n",
    "    \"f1\":    [mv[2], ma[2], sv[2], sa[2]],\n",
    "})\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(summary.to_string(index=False))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with 9 categorical outputs (Not Binary)\n",
    "\n",
    "=== Summary ===\n",
    "               \n",
    "               model      acc     bacc       f1\n",
    "\n",
    "\n",
    "    multi_output:valence 0.121094 0.130769 0.102354\n",
    "\n",
    "\n",
    "    multi_output:arousal 0.132812 0.203067 0.120390\n",
    "\n",
    "\n",
    "      single:valence 0.183594 0.140942 0.126499\n",
    "\n",
    "      \n",
    "      single:arousal 0.175781 0.251657 0.168949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1280, 96)\n",
      "Valence class counts (1..9): [221 249 217 224 173  66  88  36   6]\n",
      "Arousal class counts (1..9): [141 238 284 240 116 124  79  39  19]\n",
      "=== Multi-output MLP (9-class) ===\n",
      "Valence: acc=0.168, bacc=0.173, f1_macro=0.144\n",
      "Arousal: acc=0.168, bacc=0.232, f1_macro=0.153\n",
      "=== Single-output MLP (9-class) — Valence ===\n",
      "Valence: acc=0.156, bacc=0.152, f1_macro=0.128\n",
      "=== Single-output MLP (9-class) — Arousal ===\n",
      "Arousal: acc=0.180, bacc=0.224, f1_macro=0.154\n",
      "\n",
      "=== Summary ===\n",
      "               model      acc     bacc       f1\n",
      "multi_output:valence 0.167969 0.173329 0.143822\n",
      "multi_output:arousal 0.167969 0.232476 0.153016\n",
      "      single:valence 0.156250 0.152031 0.127518\n",
      "      single:arousal 0.179688 0.223724 0.154343\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N_CLASSES = 9  # categories 1..9\n",
    "\n",
    "def load_subject_features(subj_id):\n",
    "    \"\"\"Concatenate alpha/beta/gamma for 32 channels -> (40, 96)\"\"\"\n",
    "    per_ch = []\n",
    "    for ch in EEG_CH_NAMES:\n",
    "        csv_path = os.path.join(features_base, ch, f\"s{subj_id:02d}_bandpower.csv\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        per_ch.append(df[['alpha_power','beta_power','gamma_power']].values)  # (40,3)\n",
    "    X_subj = np.hstack(per_ch)  # (40, 96)\n",
    "    return X_subj\n",
    "\n",
    "def load_subject_labels_rounded(subj_id):\n",
    "    \"\"\"\n",
    "    Load continuous labels (1..9) and round to nearest int in [1..9],\n",
    "    returning class indices in [0..8] (for PyTorch CrossEntropyLoss).\n",
    "    \"\"\"\n",
    "    with open(os.path.join(base_path_dat, f\"s{subj_id:02d}.dat\"), \"rb\") as f:\n",
    "        raw = pickle.load(f, encoding=\"latin1\")\n",
    "    labels = raw['labels'].astype(np.float32)  # (40,4)\n",
    "    v = labels[:, 0]  # valence\n",
    "    a = labels[:, 1]  # arousal\n",
    "    # Round to 1..9 categories, then map to 0..8\n",
    "    v_cat = np.clip(np.rint(v).astype(int), 1, 9) - 1\n",
    "    a_cat = np.clip(np.rint(a).astype(int), 1, 9) - 1\n",
    "    return v_cat, a_cat  # (40,), (40,)\n",
    "\n",
    "# --------------------\n",
    "# Build X, y with guaranteed alignment\n",
    "# --------------------\n",
    "X_list, yv_list, ya_list = [], [], []\n",
    "for s in SUBJECTS:\n",
    "    Xs = load_subject_features(s)                  # (40,96)\n",
    "    yv_s, ya_s = load_subject_labels_rounded(s)    # (40,), (40,) in 0..8\n",
    "    assert Xs.shape[0] == TRIALS_PER_SUBJ == yv_s.shape[0] == ya_s.shape[0]\n",
    "    X_list.append(Xs)\n",
    "    yv_list.append(yv_s)\n",
    "    ya_list.append(ya_s)\n",
    "\n",
    "X = np.vstack(X_list)                   # (1280, 96)\n",
    "y_valence = np.hstack(yv_list)          # (1280,) 0..8\n",
    "y_arousal = np.hstack(ya_list)          # (1280,) 0..8\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Valence class counts (1..9):\", np.bincount(y_valence, minlength=N_CLASSES))\n",
    "print(\"Arousal class counts (1..9):\", np.bincount(y_arousal, minlength=N_CLASSES))\n",
    "\n",
    "# --------------------\n",
    "# Split + scale\n",
    "# --------------------\n",
    "# Stratify on valence (you can stratify on arousal instead; for a stricter setup use GroupKFold by subject)\n",
    "X_train, X_test, yv_train, yv_test, ya_train, ya_test = train_test_split(\n",
    "    X, y_valence, y_arousal, test_size=0.2, random_state=42, stratify=y_valence\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "yv_train_tensor = torch.tensor(yv_train, dtype=torch.long)\n",
    "yv_test_tensor  = torch.tensor(yv_test,  dtype=torch.long)\n",
    "ya_train_tensor = torch.tensor(ya_train, dtype=torch.long)\n",
    "ya_test_tensor  = torch.tensor(ya_test,  dtype=torch.long)\n",
    "\n",
    "train_ds = TensorDataset(X_train_tensor, yv_train_tensor, ya_train_tensor)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# --------------------\n",
    "# Optional: class weights (handle rare categories better)\n",
    "# --------------------\n",
    "def make_class_weights(y, n_classes=N_CLASSES):\n",
    "    counts = np.bincount(y, minlength=n_classes).astype(np.float32)\n",
    "    # inverse frequency; avoid division by zero\n",
    "    w = 1.0 / np.maximum(counts, 1.0)\n",
    "    w = w * (n_classes / w.sum())  # normalize for stability\n",
    "    return torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "w_val = make_class_weights(yv_train)\n",
    "w_aro = make_class_weights(ya_train)\n",
    "\n",
    "# --------------------\n",
    "# Models\n",
    "# --------------------\n",
    "class MultiOutputMLP(nn.Module):\n",
    "    def __init__(self, input_dim=96, h1=256, h2=128, p=0.3, n_classes=N_CLASSES):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, h1)\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "        self.do  = nn.Dropout(p)\n",
    "        self.act = nn.ReLU()\n",
    "        self.out_v = nn.Linear(h2, n_classes)\n",
    "        self.out_a = nn.Linear(h2, n_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.do(x)\n",
    "        x = self.act(self.fc2(x))\n",
    "        return self.out_v(x), self.out_a(x)\n",
    "\n",
    "class SingleOutputMLP(nn.Module):\n",
    "    def __init__(self, input_dim=96, h1=256, h2=128, p=0.3, n_classes=N_CLASSES):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, h1)\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "        self.do  = nn.Dropout(p)\n",
    "        self.act = nn.ReLU()\n",
    "        self.out = nn.Linear(h2, n_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.do(x)\n",
    "        x = self.act(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "def report(name, y_true, y_pred):\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    bacc = balanced_accuracy_score(y_true, y_pred)\n",
    "    f1   = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    print(f\"{name}: acc={acc:.3f}, bacc={bacc:.3f}, f1_macro={f1:.3f}\")\n",
    "    return acc, bacc, f1\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EPOCHS = 40\n",
    "\n",
    "# --------------------\n",
    "# Train multi-output (two 9-way heads)\n",
    "# --------------------\n",
    "model_multi = MultiOutputMLP().to(device)\n",
    "crit_v = nn.CrossEntropyLoss(weight=w_val.to(device))\n",
    "crit_a = nn.CrossEntropyLoss(weight=w_aro.to(device))\n",
    "opt  = optim.Adam(model_multi.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "for _ in range(EPOCHS):\n",
    "    model_multi.train()\n",
    "    for xb, yv, ya in train_loader:\n",
    "        xb, yv, ya = xb.to(device), yv.to(device), ya.to(device)\n",
    "        opt.zero_grad()\n",
    "        out_v, out_a = model_multi(xb)\n",
    "        loss = crit_v(out_v, yv) + crit_a(out_a, ya)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "model_multi.eval()\n",
    "with torch.no_grad():\n",
    "    out_v, out_a = model_multi(X_test_tensor.to(device))\n",
    "    yv_pred = torch.argmax(out_v, 1).cpu().numpy()\n",
    "    ya_pred = torch.argmax(out_a, 1).cpu().numpy()\n",
    "\n",
    "print(\"=== Multi-output MLP (9-class) ===\")\n",
    "mv = report(\"Valence\", yv_test, yv_pred)\n",
    "ma = report(\"Arousal\", ya_test, ya_pred)\n",
    "\n",
    "# --------------------\n",
    "# Train & eval separate MLPs (9-class each)\n",
    "# --------------------\n",
    "def train_single(Xtr, ytr, Xte, yte, label_name, w_cls):\n",
    "    model = SingleOutputMLP().to(device)\n",
    "    crit  = nn.CrossEntropyLoss(weight=w_cls.to(device))\n",
    "    opt   = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    ds = TensorDataset(torch.tensor(Xtr, dtype=torch.float32),\n",
    "                       torch.tensor(ytr, dtype=torch.long))\n",
    "    dl = DataLoader(ds, batch_size=64, shuffle=True)\n",
    "    for _ in range(EPOCHS):\n",
    "        model.train()\n",
    "        for xb, yb in dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.tensor(Xte, dtype=torch.float32).to(device))\n",
    "        y_pred = torch.argmax(logits, 1).cpu().numpy()\n",
    "    print(f\"=== Single-output MLP (9-class) — {label_name} ===\")\n",
    "    return report(label_name, yte, y_pred)\n",
    "\n",
    "sv = train_single(X_train, yv_train, X_test, yv_test, \"Valence\", w_val)\n",
    "sa = train_single(X_train, ya_train, X_test, ya_test, \"Arousal\", w_aro)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"model\": [\"multi_output:valence\",\"multi_output:arousal\",\"single:valence\",\"single:arousal\"],\n",
    "    \"acc\":   [mv[0], ma[0], sv[0], sa[0]],\n",
    "    \"bacc\":  [mv[1], ma[1], sv[1], sa[1]],\n",
    "    \"f1\":    [mv[2], ma[2], sv[2], sa[2]],\n",
    "})\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (v3.10.9:1dd9be6584, Dec  6 2022, 14:37:36) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
