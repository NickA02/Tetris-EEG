{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, math, warnings\n",
    "import numpy as np\n",
    "import h5py\n",
    "from scipy.signal import welch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, warnings\n",
    "import numpy as np\n",
    "from scipy.signal import welch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "DATA_DIR = \"data\"  # folder containing s01.dat ... s32.dat\n",
    "SUBJECTS = [f\"s{idx:02d}.dat\" for idx in range(1, 33)]\n",
    "\n",
    "FS = 128\n",
    "TRIAL_SEC = 60\n",
    "BASELINE_SEC = 3\n",
    "KEEP_SAMPLES = FS * TRIAL_SEC              # 7680\n",
    "DROP_SAMPLES = FS * BASELINE_SEC           # 384 (drop from start)\n",
    "\n",
    "EEG_CH = 32                                # first 32 channels are EEG\n",
    "BANDS = [(4,8), (8,13), (13,30), (30,45)]  # theta, alpha, beta, gamma\n",
    "LABELS = [\"valence\", \"arousal\", \"dominance\", \"liking\"]\n",
    "TARGET = \"valence\"                         # change to \"arousal\"/\"liking\"/\"dominance\" as needed\n",
    "\n",
    "# We'll use 4 robust peripheral channels commonly present after EEG:\n",
    "#  GSR, Respiration, BVP, Skin Temp (indices relative to *peripheral* block)\n",
    "PERIPH_KEEP = [0, 1, 2, 3]\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def binarize(y_cont):\n",
    "    \"\"\"DEAP convention: rating > 5 => 1 (High), else 0 (Low).\"\"\"\n",
    "    return (y_cont > 5.0).astype(np.int32)\n",
    "\n",
    "def load_subject_dat(path):\n",
    "    \"\"\"\n",
    "    Load DEAP preprocessed .dat (pickle).\n",
    "    Returns:\n",
    "      eeg:    (40, 32, 7680)\n",
    "      periph: (40, P, 7680) or None\n",
    "      labels: (40, 4)\n",
    "    \"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        obj = pickle.load(f, encoding=\"latin1\")  # DEAP pickles were py2\n",
    "    data = obj[\"data\"]         # shape (40, 40, 8064): trials, channels, samples\n",
    "    labels = obj[\"labels\"]     # shape (40, 4): valence, arousal, dominance, liking (1..9)\n",
    "\n",
    "    # split channels\n",
    "    eeg = data[:, :EEG_CH, :]                  # (40, 32, 8064)\n",
    "    periph = data[:, EEG_CH:, :]               # (40, 8, 8064)\n",
    "\n",
    "    # keep only the 60s trial segment (drop first 3s baseline)\n",
    "    eeg = eeg[:, :, DROP_SAMPLES:DROP_SAMPLES + KEEP_SAMPLES]          # (40, 32, 7680)\n",
    "    periph = periph[:, :, DROP_SAMPLES:DROP_SAMPLES + KEEP_SAMPLES]    # (40, 8, 7680)\n",
    "\n",
    "    # keep robust peripheral subset\n",
    "    if periph.shape[1] >= 4:\n",
    "        periph = periph[:, PERIPH_KEEP, :]\n",
    "    else:\n",
    "        periph = None\n",
    "\n",
    "    return eeg.astype(np.float32), (periph.astype(np.float32) if periph is not None else None), labels.astype(np.float32)\n",
    "\n",
    "def welch_logbp(x, fs=FS, nperseg=256, noverlap=128):\n",
    "    f, Pxx = welch(x, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "    out = []\n",
    "    for lo, hi in BANDS:\n",
    "        m = (f >= lo) & (f <= hi)\n",
    "        bp = np.trapz(Pxx[m], f[m]) + 1e-12\n",
    "        out.append(np.log(bp))\n",
    "    return np.asarray(out, dtype=np.float32)\n",
    "\n",
    "def eeg_features(trial_eeg):\n",
    "    # trial_eeg: (32, 7680)\n",
    "    feats = [welch_logbp(trial_eeg[ch]) for ch in range(trial_eeg.shape[0])]\n",
    "    return np.concatenate(feats, axis=0)  # (32 * 4,)\n",
    "\n",
    "def periph_features(trial_periph):\n",
    "    # trial_periph: (P, 7680); simple trial-level stats\n",
    "    def slope(x):\n",
    "        t = np.arange(x.size, dtype=np.float32)\n",
    "        A = np.vstack([t, np.ones_like(t)]).T\n",
    "        m, _ = np.linalg.lstsq(A, x, rcond=None)[0]\n",
    "        return np.float32(m)\n",
    "    feats = []\n",
    "    for ch in range(trial_periph.shape[0]):\n",
    "        sig = trial_periph[ch]\n",
    "        feats.extend([np.mean(sig), np.std(sig), slope(sig)])\n",
    "    return np.asarray(feats, dtype=np.float32)  # (P*3,)\n",
    "\n",
    "def build_features(eeg, periph):\n",
    "    X_eeg = np.vstack([eeg_features(eeg[i]) for i in range(eeg.shape[0])])\n",
    "    X_per = None\n",
    "    if periph is not None:\n",
    "        X_per = np.vstack([periph_features(periph[i]) for i in range(periph.shape[0])])\n",
    "    return X_eeg, X_per\n",
    "\n",
    "def metric_dict(y_true, y_pred):\n",
    "    return {\n",
    "        \"acc\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"bacc\": float(balanced_accuracy_score(y_true, y_pred)),\n",
    "        \"f1_macro\": float(f1_score(y_true, y_pred, average=\"macro\"))\n",
    "    }\n",
    "\n",
    "def subject_dependent_cv(X, y, C=1.0):\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    for tr, te in skf.split(X, y):\n",
    "        scaler = StandardScaler()\n",
    "        Xtr = scaler.fit_transform(X[tr])\n",
    "        Xte = scaler.transform(X[te])\n",
    "        base = LinearSVC(C=C)\n",
    "        clf = CalibratedClassifierCV(base, method=\"sigmoid\", cv=3)\n",
    "        clf.fit(Xtr, y[tr])\n",
    "        yhat = clf.predict(Xte)\n",
    "        scores.append(metric_dict(y[te], yhat))\n",
    "    return {k: float(np.mean([s[k] for s in scores])) for k in scores[0]}\n",
    "\n",
    "def loso(mods, labels, C=1.0):\n",
    "    \"\"\"\n",
    "    mods: dict modality -> list of per-subject X arrays\n",
    "    labels: list of per-subject y arrays\n",
    "    returns averaged metrics per modality and for fusion\n",
    "    \"\"\"\n",
    "    subj_n = len(labels)\n",
    "    res = {k: [] for k in mods.keys()}\n",
    "    res[\"fusion\"] = []\n",
    "\n",
    "    for te in range(subj_n):\n",
    "        # set up per-modality models on train subjects\n",
    "        permod = {}\n",
    "        for mod, Xs in mods.items():\n",
    "            Xtr = np.vstack([Xs[i] for i in range(subj_n) if i != te])\n",
    "            ytr = np.hstack([labels[i] for i in range(subj_n) if i != te])\n",
    "            Xte = Xs[te]\n",
    "            yte = labels[te]\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            Xtr = scaler.fit_transform(Xtr)\n",
    "            Xte = scaler.transform(Xte)\n",
    "\n",
    "            base = LinearSVC(C=C)\n",
    "            clf = CalibratedClassifierCV(base, method=\"sigmoid\", cv=3)\n",
    "            clf.fit(Xtr, ytr)\n",
    "            yhat = clf.predict(Xte)\n",
    "            proba = clf.predict_proba(Xte)\n",
    "            permod[mod] = (yte, yhat, proba)\n",
    "\n",
    "        # per-modality metrics\n",
    "        for mod, (yte, yhat, proba) in permod.items():\n",
    "            res[mod].append(metric_dict(yte, yhat))\n",
    "\n",
    "        # simple late fusion: average calibrated probabilities\n",
    "        probs = np.mean(np.stack([v[2] for v in permod.values()], axis=0), axis=0)\n",
    "        yhat_f = np.argmax(probs, axis=1)\n",
    "        yte0 = next(iter(permod.values()))[0]\n",
    "        res[\"fusion\"].append(metric_dict(yte0, yhat_f))\n",
    "\n",
    "    return {k: {m: float(np.mean([r[m] for r in v])) for m in v[0]} for k, v in res.items()}\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    all_eeg, all_per, all_y = [], [], []\n",
    "\n",
    "    print(\"Loading .dat files and extracting features …\")\n",
    "    for fname in tqdm(SUBJECTS):\n",
    "        fpath = os.path.join(DATA_DIR, fname)\n",
    "        if not os.path.exists(fpath):\n",
    "            continue\n",
    "        eeg, periph, labels = load_subject_dat(fpath)\n",
    "        Xeeg, Xper = build_features(eeg, periph)\n",
    "\n",
    "        # choose target label and binarize\n",
    "        t_idx = LABELS.index(TARGET)\n",
    "        y = binarize(labels[:, t_idx])\n",
    "\n",
    "        all_eeg.append(Xeeg)\n",
    "        if Xper is not None:\n",
    "            all_per.append(Xper)\n",
    "        else:\n",
    "            all_per.append(np.zeros((Xeeg.shape[0], 0), dtype=np.float32))\n",
    "        all_y.append(y)\n",
    "\n",
    "    # ---- Subject-dependent baseline (EEG only) ----\n",
    "    sub_metrics = []\n",
    "    for i in range(len(all_eeg)):\n",
    "        sub_metrics.append(subject_dependent_cv(all_eeg[i], all_y[i], C=1.0))\n",
    "    avg_sub = {k: float(np.mean([m[k] for m in sub_metrics])) for k in sub_metrics[0]}\n",
    "    print(\"\\n=== Subject-dependent (EEG) — 10-fold within subject ===\")\n",
    "    print(avg_sub)\n",
    "\n",
    "    # ---- LOSO across subjects (EEG, peripherals, fusion) ----\n",
    "    mods = {\"eeg\": all_eeg, \"periph\": all_per}\n",
    "    loso_avg = loso(mods, all_y, C=1.0)\n",
    "    print(\"\\n=== LOSO Cross-Subject ===\")\n",
    "    for k, v in loso_avg.items():\n",
    "        print(k, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "DATA_DIR = \"datasets/DEAP/deap-dataset/data_preprocessed_python\"\n",
    "FS = 128                           # preprocessed sampling rate\n",
    "EEG_CH_COUNT = 32\n",
    "TRIAL_LEN_SEC = 60\n",
    "TRIAL_SAMPLES = FS * TRIAL_LEN_SEC # 7680\n",
    "BANDS = [(4,8), (8,13), (13,30), (30,45)]  # theta, alpha, beta, gamma\n",
    "LABEL_NAMES = [\"valence\", \"arousal\", \"dominance\", \"liking\"]\n",
    "TARGET = \"valence\"                 # change to \"arousal\" or \"liking\" as needed\n",
    "SUBJECTS = [f\"s{idx:02d}.dat\" for idx in range(1,33)]\n",
    "\n",
    "# Peripheral channels (DEAP preprocessed order reference)\n",
    "# EEG: 32 channels first; then 8 peripherals in common mirrors:\n",
    "#  32: GSR, 33: Respiration belt, 34: Plethysmograph (BVP),\n",
    "#  35: Temp, 36-39: EOG/EMG variants depending on mirror (often 4 channels).\n",
    "# We’ll pick the core 4 peripherals that are consistent across mirrors.\n",
    "PERIPH_IDXS = None  # will set after we parse a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def binarize_ratings(y_cont):\n",
    "    # DEAP convention used widely: >5 => High; else Low\n",
    "    return (y_cont > 5.0).astype(int)\n",
    "\n",
    "def load_subject_mat(path):\n",
    "    \"\"\"\n",
    "    Loads a DEAP preprocessed subject .mat (HDF5 or MATLAB v7.3).\n",
    "    Returns:\n",
    "      eeg: shape (40, 32, 8064)\n",
    "      periph: shape (40, P, 8064) with P peripheral channels if available else None\n",
    "      labels: shape (40, 4)\n",
    "    \"\"\"\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        # DEAP files vary between mirrors; try common layouts\n",
    "        if 'data' in f:\n",
    "            d = np.array(f['data'])\n",
    "            # Many mirrors store as (40 trials, 40 channels, 8064 samples)\n",
    "            # or (40, 32, 8064) + peripherals after 32.\n",
    "            if d.ndim == 3:\n",
    "                trials, chans, samples = d.shape\n",
    "                eeg = d[:, :EEG_CH_COUNT, :]\n",
    "                periph = d[:, EEG_CH_COUNT:, :] if chans > EEG_CH_COUNT else None\n",
    "            elif d.ndim == 2:\n",
    "                # Rare: flattened; not common — skip\n",
    "                raise RuntimeError(\"Unexpected 2D data array\")\n",
    "            else:\n",
    "                raise RuntimeError(\"Unknown data layout in .mat\")\n",
    "        else:\n",
    "            raise RuntimeError(\"No 'data' key found in .mat file\")\n",
    "\n",
    "        if 'labels' in f:\n",
    "            labels = np.array(f['labels'])\n",
    "        elif 'label' in f:\n",
    "            labels = np.array(f['label'])\n",
    "        else:\n",
    "            raise RuntimeError(\"No labels found in .mat\")\n",
    "\n",
    "        # labels may be (4,40) — transpose to (40,4)\n",
    "        if labels.shape[0] == 4 and labels.shape[1] == 40:\n",
    "            labels = labels.T\n",
    "        return eeg.astype(np.float32), (periph.astype(np.float32) if periph is not None else None), labels.astype(np.float32)\n",
    "\n",
    "def welch_logpsd(x, fs=128, nperseg=256, noverlap=128):\n",
    "    \"\"\"\n",
    "    x: (samples,) 1D signal\n",
    "    returns log bandpowers for BANDS\n",
    "    \"\"\"\n",
    "    f, Pxx = welch(x, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "    feat = []\n",
    "    for (lo, hi) in BANDS:\n",
    "        mask = (f >= lo) & (f <= hi)\n",
    "        bp = np.trapz(Pxx[mask], f[mask]) + 1e-12\n",
    "        feat.append(np.log(bp))\n",
    "    return np.array(feat, dtype=np.float32)\n",
    "\n",
    "def extract_eeg_features(trial_eeg):\n",
    "    \"\"\"\n",
    "    trial_eeg: (32, samples)\n",
    "    return: (32 * len(BANDS),) bandpower features\n",
    "    \"\"\"\n",
    "    feats = [welch_logpsd(trial_eeg[ch]) for ch in range(trial_eeg.shape[0])]\n",
    "    return np.concatenate(feats, axis=0)\n",
    "\n",
    "def extract_periph_features(trial_periph):\n",
    "    \"\"\"\n",
    "    trial_periph: (P, samples)\n",
    "    Simple stats per channel (mean, std, slope, peakrate for BVP/GSR-ish)\n",
    "    \"\"\"\n",
    "    def slope(x):\n",
    "        t = np.arange(len(x))\n",
    "        # least squares slope\n",
    "        A = np.vstack([t, np.ones_like(t)]).T\n",
    "        m, _ = np.linalg.lstsq(A, x, rcond=None)[0]\n",
    "        return m\n",
    "\n",
    "    feats = []\n",
    "    for ch in range(trial_periph.shape[0]):\n",
    "        sig = trial_periph[ch]\n",
    "        feats.extend([np.mean(sig), np.std(sig), slope(sig)])\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "def build_modality_features(eeg, periph):\n",
    "    X_eeg = np.vstack([extract_eeg_features(eeg[i]) for i in range(eeg.shape[0])])\n",
    "    X_per = None\n",
    "    if periph is not None and periph.shape[1] > 0:\n",
    "        X_per = np.vstack([extract_periph_features(periph[i]) for i in range(periph.shape[0])])\n",
    "    return X_eeg, X_per\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"acc\": accuracy_score(y_true, y_pred),\n",
    "        \"bacc\": balanced_accuracy_score(y_true, y_pred),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\")\n",
    "    }\n",
    "\n",
    "def train_eval_subject_dependent(X, y):\n",
    "    \"\"\"\n",
    "    10-fold CV within subject (40 trials) -> average metrics\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    mm = []\n",
    "    for tr, te in skf.split(X, y):\n",
    "        scaler = StandardScaler()\n",
    "        Xtr = scaler.fit_transform(X[tr])\n",
    "        Xte = scaler.transform(X[te])\n",
    "        base = LinearSVC(C=1.0)\n",
    "        clf = CalibratedClassifierCV(base, method=\"sigmoid\", cv=3)\n",
    "        clf.fit(Xtr, y[tr])\n",
    "        yhat = clf.predict(Xte)\n",
    "        mm.append(metrics(y[te], yhat))\n",
    "    # average\n",
    "    out = {k: float(np.mean([m[k] for m in mm])) for k in mm[0]}\n",
    "    return out\n",
    "\n",
    "def train_eval_LOSO(mods, labels):\n",
    "    \"\"\"\n",
    "    Leave-one-subject-out across 32 subjects. mods is a dict of modality→list_per_subject_of_X\n",
    "    labels: list_per_subject_of_y\n",
    "    - Trains calibrated LinearSVC per modality on 31 subjects\n",
    "    - Decision-level fusion: average predicted probabilities\n",
    "    \"\"\"\n",
    "    subjects = len(labels)\n",
    "    results = {k: [] for k in mods.keys()}\n",
    "    results[\"fusion\"] = []\n",
    "\n",
    "    for test_idx in range(subjects):\n",
    "        # Build train/test for each modality\n",
    "        per_mod_train = {}\n",
    "        per_mod_test  = {}\n",
    "        for mod, X_list in mods.items():\n",
    "            Xtr = np.vstack([X_list[i] for i in range(subjects) if i != test_idx])\n",
    "            Xte = X_list[test_idx]\n",
    "            ytr = np.hstack([labels[i] for i in range(subjects) if i != test_idx])\n",
    "            yte = labels[test_idx]\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            Xtr = scaler.fit_transform(Xtr)\n",
    "            Xte = scaler.transform(Xte)\n",
    "\n",
    "            base = LinearSVC(C=1.0)\n",
    "            clf = CalibratedClassifierCV(base, method=\"sigmoid\", cv=3)\n",
    "            clf.fit(Xtr, ytr)\n",
    "            yhat = clf.predict(Xte)\n",
    "            proba = clf.predict_proba(Xte)  # (N,2)\n",
    "\n",
    "            per_mod_train[mod] = (clf, ytr)  # keep if needed\n",
    "            per_mod_test[mod]  = (yte, yhat, proba)\n",
    "\n",
    "        # per-modality metrics\n",
    "        for mod, (yte, yhat, proba) in per_mod_test.items():\n",
    "            results[mod].append(metrics(yte, yhat))\n",
    "\n",
    "        # fusion: average probs over available modalities\n",
    "        probas = [v[2] for v in per_mod_test.values()]\n",
    "        P = np.mean(np.stack(probas, axis=0), axis=0)\n",
    "        yhat_fus = np.argmax(P, axis=1)\n",
    "        yte0 = list(per_mod_test.values())[0][0]\n",
    "        results[\"fusion\"].append(metrics(yte0, yhat_fus))\n",
    "\n",
    "    # average metrics\n",
    "    avg = {k: {m: float(np.mean([r[m] for r in v])) for m in v[0]} for k, v in results.items()}\n",
    "    return avg\n",
    "\n",
    "# -----------------------------\n",
    "# Main: build subject feature sets and run experiments\n",
    "# -----------------------------\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "all_eeg, all_per, all_y = [], [], []\n",
    "\n",
    "print(\"Loading subjects & extracting features …\")\n",
    "for sname in tqdm(SUBJECTS):\n",
    "    fpath = os.path.join(DATA_DIR, sname)\n",
    "    if not os.path.exists(fpath):\n",
    "        continue\n",
    "    eeg, periph, labels = load_subject_mat(fpath)\n",
    "\n",
    "    # normalize shapes: many mirrors store (40, channels, samples)\n",
    "    if eeg.shape[-1] != TRIAL_SAMPLES and eeg.shape[-1] > TRIAL_SAMPLES:\n",
    "        # Some mirrors include 3 s baseline (384 samples) before the 60 s trial.\n",
    "        # Keep last 60 s.\n",
    "        eeg = eeg[..., -TRIAL_SAMPLES:]\n",
    "        if periph is not None:\n",
    "            periph = periph[..., -TRIAL_SAMPLES:]\n",
    "\n",
    "    # choose peripheral indices if present\n",
    "    if periph is not None and PERIPH_IDXS is None:\n",
    "        # Try to keep first 4 peripheral channels (GSR, Resp, BVP, Temp)—robust across mirrors\n",
    "        P = periph.shape[1]\n",
    "        PERIPH_IDXS = list(range(min(P, 4)))\n",
    "\n",
    "    # build features per trial\n",
    "    Xeeg, Xper = build_modality_features(eeg, (periph[:, PERIPH_IDXS, :] if (periph is not None and len(PERIPH_IDXS)>0) else None))\n",
    "    all_eeg.append(Xeeg)\n",
    "    all_per.append(Xper if Xper is not None else np.zeros((eeg.shape[0], 0), dtype=np.float32))\n",
    "\n",
    "    # labels → pick target and binarize\n",
    "    idx = LABEL_NAMES.index(TARGET)\n",
    "    y = binarize_ratings(labels[:, idx])\n",
    "    all_y.append(y)\n",
    "\n",
    "# Subject-dependent (per subject) EEG baseline (classic)\n",
    "subdep_metrics = []\n",
    "for i in range(len(all_eeg)):\n",
    "    mm = train_eval_subject_dependent(all_eeg[i], all_y[i])\n",
    "    subdep_metrics.append(mm)\n",
    "avg_subdep = {k: float(np.mean([m[k] for m in subdep_metrics])) for k in subdep_metrics[0]}\n",
    "\n",
    "# LOSO with EEG, peripherals, and fusion\n",
    "mods = {\"eeg\": all_eeg, \"periph\": all_per}\n",
    "loso_avg = train_eval_LOSO(mods, all_y)\n",
    "\n",
    "print(\"\\n=== Subject-dependent (EEG) — 10-fold within subject ===\")\n",
    "print(avg_subdep)\n",
    "\n",
    "print(\"\\n=== LOSO Cross-Subject ===\")\n",
    "for k, v in loso_avg.items():\n",
    "    print(k, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subjects & extracting features …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/DEAP/deap-dataset/data_preprocessed_python/s01.dat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to synchronously open file (file signature not found)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(fpath):\n\u001b[1;32m     12\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m eeg, periph, labels \u001b[39m=\u001b[39m load_subject_mat(fpath)\n\u001b[1;32m     15\u001b[0m \u001b[39m# normalize shapes: many mirrors store (40, channels, samples)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mif\u001b[39;00m eeg\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m TRIAL_SAMPLES \u001b[39mand\u001b[39;00m eeg\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m TRIAL_SAMPLES:\n\u001b[1;32m     17\u001b[0m     \u001b[39m# Some mirrors include 3 s baseline (384 samples) before the 60 s trial.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[39m# Keep last 60 s.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m, in \u001b[0;36mload_subject_mat\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mload_subject_mat\u001b[39m(path):\n\u001b[1;32m      9\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m    Loads a DEAP preprocessed subject .mat (HDF5 or MATLAB v7.3).\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m      labels: shape (40, 4)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     \u001b[39mwith\u001b[39;00m h5py\u001b[39m.\u001b[39;49mFile(path, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     17\u001b[0m         \u001b[39m# DEAP files vary between mirrors; try common layouts\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m f:\n\u001b[1;32m     19\u001b[0m             d \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(f[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/h5py/_hl/files.py:564\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    555\u001b[0m     fapl \u001b[39m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    556\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    557\u001b[0m                      alignment_threshold\u001b[39m=\u001b[39malignment_threshold,\n\u001b[1;32m    558\u001b[0m                      alignment_interval\u001b[39m=\u001b[39malignment_interval,\n\u001b[1;32m    559\u001b[0m                      meta_block_size\u001b[39m=\u001b[39mmeta_block_size,\n\u001b[1;32m    560\u001b[0m                      \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    561\u001b[0m     fcpl \u001b[39m=\u001b[39m make_fcpl(track_order\u001b[39m=\u001b[39mtrack_order, fs_strategy\u001b[39m=\u001b[39mfs_strategy,\n\u001b[1;32m    562\u001b[0m                      fs_persist\u001b[39m=\u001b[39mfs_persist, fs_threshold\u001b[39m=\u001b[39mfs_threshold,\n\u001b[1;32m    563\u001b[0m                      fs_page_size\u001b[39m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 564\u001b[0m     fid \u001b[39m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[39m=\u001b[39;49mswmr)\n\u001b[1;32m    566\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(libver, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    567\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_libver \u001b[39m=\u001b[39m libver\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/h5py/_hl/files.py:238\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[39mif\u001b[39;00m swmr \u001b[39mand\u001b[39;00m swmr_support:\n\u001b[1;32m    237\u001b[0m         flags \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 238\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39;49mopen(name, flags, fapl\u001b[39m=\u001b[39;49mfapl)\n\u001b[1;32m    239\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    240\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mopen(name, h5f\u001b[39m.\u001b[39mACC_RDWR, fapl\u001b[39m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:56\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:57\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to synchronously open file (file signature not found)"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Main: build subject feature sets and run experiments\n",
    "# -----------------------------\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "all_eeg, all_per, all_y = [], [], []\n",
    "\n",
    "print(\"Loading subjects & extracting features …\")\n",
    "for sname in tqdm(SUBJECTS):\n",
    "    fpath = os.path.join(DATA_DIR, sname)\n",
    "    print(fpath)\n",
    "    if not os.path.exists(fpath):\n",
    "        continue\n",
    "    eeg, periph, labels = load_subject_mat(fpath)\n",
    "\n",
    "    # normalize shapes: many mirrors store (40, channels, samples)\n",
    "    if eeg.shape[-1] != TRIAL_SAMPLES and eeg.shape[-1] > TRIAL_SAMPLES:\n",
    "        # Some mirrors include 3 s baseline (384 samples) before the 60 s trial.\n",
    "        # Keep last 60 s.\n",
    "        eeg = eeg[..., -TRIAL_SAMPLES:]\n",
    "        if periph is not None:\n",
    "            periph = periph[..., -TRIAL_SAMPLES:]\n",
    "\n",
    "    # choose peripheral indices if present\n",
    "    if periph is not None and PERIPH_IDXS is None:\n",
    "        # Try to keep first 4 peripheral channels (GSR, Resp, BVP, Temp)—robust across mirrors\n",
    "        P = periph.shape[1]\n",
    "        PERIPH_IDXS = list(range(min(P, 4)))\n",
    "\n",
    "    # build features per trial\n",
    "    Xeeg, Xper = build_modality_features(eeg, (periph[:, PERIPH_IDXS, :] if (periph is not None and len(PERIPH_IDXS)>0) else None))\n",
    "    all_eeg.append(Xeeg)\n",
    "    all_per.append(Xper if Xper is not None else np.zeros((eeg.shape[0], 0), dtype=np.float32))\n",
    "\n",
    "    # labels → pick target and binarize\n",
    "    idx = LABEL_NAMES.index(TARGET)\n",
    "    y = binarize_ratings(labels[:, idx])\n",
    "    all_y.append(y)\n",
    "\n",
    "# Subject-dependent (per subject) EEG baseline (classic)\n",
    "subdep_metrics = []\n",
    "for i in range(len(all_eeg)):\n",
    "    mm = train_eval_subject_dependent(all_eeg[i], all_y[i])\n",
    "    subdep_metrics.append(mm)\n",
    "avg_subdep = {k: float(np.mean([m[k] for m in subdep_metrics])) for k in subdep_metrics[0]}\n",
    "\n",
    "# LOSO with EEG, peripherals, and fusion\n",
    "mods = {\"eeg\": all_eeg, \"periph\": all_per}\n",
    "loso_avg = train_eval_LOSO(mods, all_y)\n",
    "\n",
    "print(\"\\n=== Subject-dependent (EEG) — 10-fold within subject ===\")\n",
    "print(avg_subdep)\n",
    "\n",
    "print(\"\\n=== LOSO Cross-Subject ===\")\n",
    "for k, v in loso_avg.items():\n",
    "    print(k, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (v3.10.9:1dd9be6584, Dec  6 2022, 14:37:36) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
