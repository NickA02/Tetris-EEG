{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Filtered Dataset with Top Important Features\n",
    "\n",
    "This notebook creates filtered datasets containing only the most important features based on the feature importance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  - Top 30 features for valence\n",
      "  - Top 30 features for arousal\n",
      "  - Top 40 features for combined (valence + arousal)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "TOP_N_VALENCE = 30  # Number of top features to use for valence prediction\n",
    "TOP_N_AROUSAL = 30  # Number of top features to use for arousal prediction\n",
    "TOP_N_COMBINED = 40  # Number of top features when combining valence & arousal\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - Top {TOP_N_VALENCE} features for valence\")\n",
    "print(f\"  - Top {TOP_N_AROUSAL} features for arousal\")\n",
    "print(f\"  - Top {TOP_N_COMBINED} features for combined (valence + arousal)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Feature Importance Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded feature importance for 96 features\n",
      "\n",
      "Top 10 features for VALENCE:\n",
      "     Feature  Valence_AvgScore\n",
      "0   AF4_beta          0.620503\n",
      "1  AF4_alpha          0.593262\n",
      "2  AF4_gamma          0.569039\n",
      "3    C3_beta          0.543337\n",
      "4   C3_alpha          0.538552\n",
      "5    O2_beta          0.476611\n",
      "6   T7_gamma          0.472280\n",
      "7   Fp1_beta          0.453665\n",
      "8   CP1_beta          0.452815\n",
      "9  CP1_alpha          0.452565\n",
      "\n",
      "Top 10 features for AROUSAL:\n",
      "     Feature  Arousal_AvgScore\n",
      "0    F7_beta          0.621710\n",
      "1    T7_beta          0.603381\n",
      "2   F7_gamma          0.581367\n",
      "3   P4_alpha          0.552088\n",
      "4   P4_gamma          0.526675\n",
      "5   T7_alpha          0.516825\n",
      "6  PO3_gamma          0.510727\n",
      "7    P4_beta          0.485583\n",
      "8   T7_gamma          0.461914\n",
      "9   F3_gamma          0.449467\n"
     ]
    }
   ],
   "source": [
    "# Load feature importance rankings\n",
    "importance_valence = pd.read_csv('feature_importance_valence.csv')\n",
    "importance_arousal = pd.read_csv('feature_importance_arousal.csv')\n",
    "\n",
    "print(f\"Loaded feature importance for {len(importance_valence)} features\")\n",
    "print(f\"\\nTop 10 features for VALENCE:\")\n",
    "print(importance_valence[['Feature', 'Valence_AvgScore']].head(10))\n",
    "\n",
    "print(f\"\\nTop 10 features for AROUSAL:\")\n",
    "print(importance_arousal[['Feature', 'Arousal_AvgScore']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels loaded: (1280, 9)\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "base_path = Path('../datasets/DEAP/deap-dataset')\n",
    "features_path = base_path / 'extracted_features'\n",
    "labels_path = base_path / 'Metadata/participant_ratings.xls'\n",
    "\n",
    "# Channel names (32 EEG channels)\n",
    "channels = [\n",
    "    'Fp1', 'AF3', 'F3', 'F7', 'FC5', 'FC1', 'C3', 'T7', 'CP5', 'CP1',\n",
    "    'P3', 'P7', 'PO3', 'O1', 'Oz', 'Pz', 'Fp2', 'AF4', 'Fz', 'F4', 'F8',\n",
    "    'FC6', 'FC2', 'Cz', 'C4', 'T8', 'CP6', 'CP2', 'P4', 'P8', 'PO4', 'O2'\n",
    "]\n",
    "\n",
    "# Frequency bands\n",
    "bands = ['alpha', 'beta', 'gamma']\n",
    "\n",
    "# Load labels\n",
    "labels_df = pd.read_excel(labels_path)\n",
    "\n",
    "# Binarize valence and arousal (threshold 4.5)\n",
    "y_valence = (labels_df['Valence'] > 4.5).astype(int).values\n",
    "y_arousal = (labels_df['Arousal'] > 4.5).astype(int).values\n",
    "\n",
    "# Also keep continuous values\n",
    "y_valence_cont = labels_df['Valence'].values\n",
    "y_arousal_cont = labels_df['Arousal'].values\n",
    "\n",
    "print(f\"Labels loaded: {labels_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature matrix shape: (1280, 96)\n",
      "Total features: 96\n"
     ]
    }
   ],
   "source": [
    "# Load and concatenate features from all channels\n",
    "X_list = []\n",
    "feature_names = []\n",
    "\n",
    "for channel in channels:\n",
    "    channel_data = []\n",
    "    \n",
    "    for subject in range(1, 33):  # 32 subjects\n",
    "        file_path = features_path / channel / f's{subject:02d}_bandpower.csv'\n",
    "        df = pd.read_csv(file_path)\n",
    "        channel_data.append(df[['alpha_power', 'beta_power', 'gamma_power']].values)\n",
    "    \n",
    "    # Stack all subjects (1280 trials total)\n",
    "    channel_features = np.vstack(channel_data)\n",
    "    X_list.append(channel_features)\n",
    "    \n",
    "    # Create feature names\n",
    "    for band in bands:\n",
    "        feature_names.append(f'{channel}_{band}')\n",
    "\n",
    "# Concatenate all channels horizontally (1280 samples × 96 features)\n",
    "X = np.hstack(X_list)\n",
    "\n",
    "print(f\"Original feature matrix shape: {X.shape}\")\n",
    "print(f\"Total features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Filtered Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VALENCE FILTERED DATASET ===\n",
      "Shape: (1280, 30)\n",
      "Selected features (30):\n",
      "   1. AF4_beta        (score: 0.6205)\n",
      "   2. AF4_alpha       (score: 0.5933)\n",
      "   3. AF4_gamma       (score: 0.5690)\n",
      "   4. C3_beta         (score: 0.5433)\n",
      "   5. C3_alpha        (score: 0.5386)\n",
      "   6. O2_beta         (score: 0.4766)\n",
      "   7. T7_gamma        (score: 0.4723)\n",
      "   8. Fp1_beta        (score: 0.4537)\n",
      "   9. CP1_beta        (score: 0.4528)\n",
      "  10. CP1_alpha       (score: 0.4526)\n",
      "  11. FC6_gamma       (score: 0.4493)\n",
      "  12. F3_alpha        (score: 0.4349)\n",
      "  13. FC6_alpha       (score: 0.4314)\n",
      "  14. Fp1_gamma       (score: 0.4221)\n",
      "  15. Fp1_alpha       (score: 0.4202)\n",
      "  16. F4_beta         (score: 0.4097)\n",
      "  17. FC1_gamma       (score: 0.3993)\n",
      "  18. P8_gamma        (score: 0.3937)\n",
      "  19. Fp2_gamma       (score: 0.3924)\n",
      "  20. FC6_beta        (score: 0.3834)\n",
      "  21. F4_gamma        (score: 0.3808)\n",
      "  22. FC2_gamma       (score: 0.3731)\n",
      "  23. P4_gamma        (score: 0.3667)\n",
      "  24. C3_gamma        (score: 0.3627)\n",
      "  25. Fp2_alpha       (score: 0.3618)\n",
      "  26. Fp2_beta        (score: 0.3608)\n",
      "  27. T8_alpha        (score: 0.3565)\n",
      "  28. C4_alpha        (score: 0.3532)\n",
      "  29. O1_alpha        (score: 0.3416)\n",
      "  30. CP1_gamma       (score: 0.3415)\n"
     ]
    }
   ],
   "source": [
    "# Get top N features for valence\n",
    "top_features_valence = importance_valence['Feature'].head(TOP_N_VALENCE).tolist()\n",
    "\n",
    "# Get indices of these features\n",
    "valence_feature_indices = [feature_names.index(f) for f in top_features_valence]\n",
    "\n",
    "# Create filtered dataset\n",
    "X_valence = X[:, valence_feature_indices]\n",
    "\n",
    "print(f\"\\n=== VALENCE FILTERED DATASET ===\")\n",
    "print(f\"Shape: {X_valence.shape}\")\n",
    "print(f\"Selected features ({len(top_features_valence)}):\")\n",
    "for i, feat in enumerate(top_features_valence, 1):\n",
    "    score = importance_valence[importance_valence['Feature'] == feat]['Valence_AvgScore'].values[0]\n",
    "    print(f\"  {i:2d}. {feat:15s} (score: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AROUSAL FILTERED DATASET ===\n",
      "Shape: (1280, 30)\n",
      "Selected features (30):\n",
      "   1. F7_beta         (score: 0.6217)\n",
      "   2. T7_beta         (score: 0.6034)\n",
      "   3. F7_gamma        (score: 0.5814)\n",
      "   4. P4_alpha        (score: 0.5521)\n",
      "   5. P4_gamma        (score: 0.5267)\n",
      "   6. T7_alpha        (score: 0.5168)\n",
      "   7. PO3_gamma       (score: 0.5107)\n",
      "   8. P4_beta         (score: 0.4856)\n",
      "   9. T7_gamma        (score: 0.4619)\n",
      "  10. F3_gamma        (score: 0.4495)\n",
      "  11. F7_alpha        (score: 0.4373)\n",
      "  12. AF4_beta        (score: 0.4300)\n",
      "  13. PO4_alpha       (score: 0.4226)\n",
      "  14. Fp1_gamma       (score: 0.4176)\n",
      "  15. C4_gamma        (score: 0.4061)\n",
      "  16. FC6_alpha       (score: 0.3821)\n",
      "  17. F3_beta         (score: 0.3812)\n",
      "  18. C4_alpha        (score: 0.3758)\n",
      "  19. Fp1_beta        (score: 0.3756)\n",
      "  20. FC2_gamma       (score: 0.3730)\n",
      "  21. FC5_beta        (score: 0.3723)\n",
      "  22. CP1_alpha       (score: 0.3716)\n",
      "  23. FC1_gamma       (score: 0.3666)\n",
      "  24. CP6_beta        (score: 0.3656)\n",
      "  25. FC2_alpha       (score: 0.3522)\n",
      "  26. CP6_gamma       (score: 0.3471)\n",
      "  27. PO4_gamma       (score: 0.3440)\n",
      "  28. F4_alpha        (score: 0.3435)\n",
      "  29. FC2_beta        (score: 0.3415)\n",
      "  30. AF3_alpha       (score: 0.3412)\n"
     ]
    }
   ],
   "source": [
    "# Get top N features for arousal\n",
    "top_features_arousal = importance_arousal['Feature'].head(TOP_N_AROUSAL).tolist()\n",
    "\n",
    "# Get indices of these features\n",
    "arousal_feature_indices = [feature_names.index(f) for f in top_features_arousal]\n",
    "\n",
    "# Create filtered dataset\n",
    "X_arousal = X[:, arousal_feature_indices]\n",
    "\n",
    "print(f\"\\n=== AROUSAL FILTERED DATASET ===\")\n",
    "print(f\"Shape: {X_arousal.shape}\")\n",
    "print(f\"Selected features ({len(top_features_arousal)}):\")\n",
    "for i, feat in enumerate(top_features_arousal, 1):\n",
    "    score = importance_arousal[importance_arousal['Feature'] == feat]['Arousal_AvgScore'].values[0]\n",
    "    print(f\"  {i:2d}. {feat:15s} (score: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMBINED (VALENCE + AROUSAL) FILTERED DATASET ===\n",
      "Shape: (1280, 40)\n",
      "Selected features (40):\n",
      "   1. AF4_beta        (combined score: 0.5253)\n",
      "   2. T7_gamma        (combined score: 0.4671)\n",
      "   3. AF4_alpha       (combined score: 0.4542)\n",
      "   4. P4_gamma        (combined score: 0.4467)\n",
      "   5. C3_alpha        (combined score: 0.4367)\n",
      "   6. P4_alpha        (combined score: 0.4305)\n",
      "   7. AF4_gamma       (combined score: 0.4207)\n",
      "   8. T7_beta         (combined score: 0.4207)\n",
      "   9. Fp1_gamma       (combined score: 0.4198)\n",
      "  10. Fp1_beta        (combined score: 0.4146)\n",
      "  11. C3_beta         (combined score: 0.4132)\n",
      "  12. CP1_alpha       (combined score: 0.4121)\n",
      "  13. FC6_alpha       (combined score: 0.4067)\n",
      "  14. F7_beta         (combined score: 0.3943)\n",
      "  15. P4_beta         (combined score: 0.3907)\n",
      "  16. PO3_gamma       (combined score: 0.3836)\n",
      "  17. FC1_gamma       (combined score: 0.3830)\n",
      "  18. F3_gamma        (combined score: 0.3804)\n",
      "  19. FC6_gamma       (combined score: 0.3740)\n",
      "  20. FC2_gamma       (combined score: 0.3730)\n",
      "  21. CP1_beta        (combined score: 0.3728)\n",
      "  22. F3_alpha        (combined score: 0.3715)\n",
      "  23. F7_gamma        (combined score: 0.3688)\n",
      "  24. C4_alpha        (combined score: 0.3645)\n",
      "  25. F4_beta         (combined score: 0.3560)\n",
      "  26. P8_gamma        (combined score: 0.3528)\n",
      "  27. O2_beta         (combined score: 0.3491)\n",
      "  28. Fp1_alpha       (combined score: 0.3478)\n",
      "  29. T7_alpha        (combined score: 0.3392)\n",
      "  30. FC6_beta        (combined score: 0.3345)\n",
      "  31. FC5_beta        (combined score: 0.3302)\n",
      "  32. F4_alpha        (combined score: 0.3292)\n",
      "  33. F3_beta         (combined score: 0.3266)\n",
      "  34. Fp2_gamma       (combined score: 0.3234)\n",
      "  35. Fp2_alpha       (combined score: 0.3182)\n",
      "  36. F4_gamma        (combined score: 0.3134)\n",
      "  37. CP6_gamma       (combined score: 0.3132)\n",
      "  38. C3_gamma        (combined score: 0.3100)\n",
      "  39. Fz_alpha        (combined score: 0.3089)\n",
      "  40. FC1_alpha       (combined score: 0.3030)\n"
     ]
    }
   ],
   "source": [
    "# Create combined dataset (union of top features for both)\n",
    "# Combine and get unique features\n",
    "combined_features = list(set(top_features_valence + top_features_arousal))\n",
    "\n",
    "# If we want exactly TOP_N_COMBINED features, rank by average of both scores\n",
    "all_importance = importance_valence[['Feature', 'Valence_AvgScore']].merge(\n",
    "    importance_arousal[['Feature', 'Arousal_AvgScore']], on='Feature'\n",
    ")\n",
    "all_importance['Combined_Score'] = (all_importance['Valence_AvgScore'] + all_importance['Arousal_AvgScore']) / 2\n",
    "all_importance = all_importance.sort_values('Combined_Score', ascending=False)\n",
    "\n",
    "top_features_combined = all_importance['Feature'].head(TOP_N_COMBINED).tolist()\n",
    "combined_feature_indices = [feature_names.index(f) for f in top_features_combined]\n",
    "\n",
    "X_combined = X[:, combined_feature_indices]\n",
    "\n",
    "print(f\"\\n=== COMBINED (VALENCE + AROUSAL) FILTERED DATASET ===\")\n",
    "print(f\"Shape: {X_combined.shape}\")\n",
    "print(f\"Selected features ({len(top_features_combined)}):\")\n",
    "for i, feat in enumerate(top_features_combined, 1):\n",
    "    score = all_importance[all_importance['Feature'] == feat]['Combined_Score'].values[0]\n",
    "    print(f\"  {i:2d}. {feat:15s} (combined score: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Filtered Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Files Saved ===\n",
      "Directory: ../datasets/DEAP/filtered_features\n",
      "\n",
      "Data files:\n",
      "  - X_valence_top30.npy\n",
      "  - X_arousal_top30.npy\n",
      "  - X_combined_top40.npy\n",
      "  - y_valence_binary.npy\n",
      "  - y_arousal_binary.npy\n",
      "  - y_valence_continuous.npy\n",
      "  - y_arousal_continuous.npy\n",
      "\n",
      "Feature lists:\n",
      "  - features_valence_top30.txt\n",
      "  - features_arousal_top30.txt\n",
      "  - features_combined_top40.txt\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('../datasets/DEAP/filtered_features')\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save as NumPy arrays (.npy)\n",
    "np.save(output_dir / 'X_valence_top30.npy', X_valence)\n",
    "np.save(output_dir / 'X_arousal_top30.npy', X_arousal)\n",
    "np.save(output_dir / 'X_combined_top40.npy', X_combined)\n",
    "np.save(output_dir / 'y_valence_binary.npy', y_valence)\n",
    "np.save(output_dir / 'y_arousal_binary.npy', y_arousal)\n",
    "np.save(output_dir / 'y_valence_continuous.npy', y_valence_cont)\n",
    "np.save(output_dir / 'y_arousal_continuous.npy', y_arousal_cont)\n",
    "\n",
    "# Save feature names as text files\n",
    "with open(output_dir / 'features_valence_top30.txt', 'w') as f:\n",
    "    f.write('\\n'.join(top_features_valence))\n",
    "\n",
    "with open(output_dir / 'features_arousal_top30.txt', 'w') as f:\n",
    "    f.write('\\n'.join(top_features_arousal))\n",
    "\n",
    "with open(output_dir / 'features_combined_top40.txt', 'w') as f:\n",
    "    f.write('\\n'.join(top_features_combined))\n",
    "\n",
    "print(\"\\n=== Files Saved ===\")\n",
    "print(f\"Directory: {output_dir}\")\n",
    "print(\"\\nData files:\")\n",
    "print(\"  - X_valence_top30.npy\")\n",
    "print(\"  - X_arousal_top30.npy\")\n",
    "print(\"  - X_combined_top40.npy\")\n",
    "print(\"  - y_valence_binary.npy\")\n",
    "print(\"  - y_arousal_binary.npy\")\n",
    "print(\"  - y_valence_continuous.npy\")\n",
    "print(\"  - y_arousal_continuous.npy\")\n",
    "print(\"\\nFeature lists:\")\n",
    "print(\"  - features_valence_top30.txt\")\n",
    "print(\"  - features_arousal_top30.txt\")\n",
    "print(\"  - features_combined_top40.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save as Pandas DataFrames (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CSV files saved:\n",
      "  - dataset_valence_top30.csv\n",
      "  - dataset_arousal_top30.csv\n",
      "  - dataset_combined_top40.csv\n",
      "\n",
      "DataFrame shapes:\n",
      "  - Valence: (1280, 34)\n",
      "  - Arousal: (1280, 34)\n",
      "  - Combined: (1280, 44)\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrames with feature names\n",
    "df_valence = pd.DataFrame(X_valence, columns=top_features_valence)\n",
    "df_valence['valence_binary'] = y_valence\n",
    "df_valence['valence_continuous'] = y_valence_cont\n",
    "df_valence['arousal_binary'] = y_arousal\n",
    "df_valence['arousal_continuous'] = y_arousal_cont\n",
    "\n",
    "df_arousal = pd.DataFrame(X_arousal, columns=top_features_arousal)\n",
    "df_arousal['valence_binary'] = y_valence\n",
    "df_arousal['valence_continuous'] = y_valence_cont\n",
    "df_arousal['arousal_binary'] = y_arousal\n",
    "df_arousal['arousal_continuous'] = y_arousal_cont\n",
    "\n",
    "df_combined = pd.DataFrame(X_combined, columns=top_features_combined)\n",
    "df_combined['valence_binary'] = y_valence\n",
    "df_combined['valence_continuous'] = y_valence_cont\n",
    "df_combined['arousal_binary'] = y_arousal\n",
    "df_combined['arousal_continuous'] = y_arousal_cont\n",
    "\n",
    "# Save as CSV\n",
    "df_valence.to_csv(output_dir / 'dataset_valence_top30.csv', index=False)\n",
    "df_arousal.to_csv(output_dir / 'dataset_arousal_top30.csv', index=False)\n",
    "df_combined.to_csv(output_dir / 'dataset_combined_top40.csv', index=False)\n",
    "\n",
    "print(\"\\nCSV files saved:\")\n",
    "print(\"  - dataset_valence_top30.csv\")\n",
    "print(\"  - dataset_arousal_top30.csv\")\n",
    "print(\"  - dataset_combined_top40.csv\")\n",
    "\n",
    "print(f\"\\nDataFrame shapes:\")\n",
    "print(f\"  - Valence: {df_valence.shape}\")\n",
    "print(f\"  - Arousal: {df_arousal.shape}\")\n",
    "print(f\"  - Combined: {df_combined.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Metadata File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata file created: README.md\n"
     ]
    }
   ],
   "source": [
    "# Create a metadata file documenting the filtered datasets\n",
    "metadata = f\"\"\"# Filtered Feature Datasets - Metadata\n",
    "\n",
    "Generated from Feature Importance Analysis\n",
    "Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Dataset Information\n",
    "\n",
    "Original dataset: DEAP (Database for Emotion Analysis using Physiological signals)\n",
    "- Total samples: 1280 (32 subjects × 40 trials)\n",
    "- Original features: 96 (32 channels × 3 frequency bands)\n",
    "- Frequency bands: alpha (8-12 Hz), beta (12-30 Hz), gamma (30-64 Hz)\n",
    "\n",
    "## Filtered Datasets\n",
    "\n",
    "### 1. Valence-Optimized Dataset\n",
    "- File: dataset_valence_top{TOP_N_VALENCE}.csv / X_valence_top{TOP_N_VALENCE}.npy\n",
    "- Features: {TOP_N_VALENCE}\n",
    "- Shape: (1280, {TOP_N_VALENCE})\n",
    "- Selection: Top {TOP_N_VALENCE} features ranked by aggregated importance for valence prediction\n",
    "\n",
    "### 2. Arousal-Optimized Dataset\n",
    "- File: dataset_arousal_top{TOP_N_AROUSAL}.csv / X_arousal_top{TOP_N_AROUSAL}.npy\n",
    "- Features: {TOP_N_AROUSAL}\n",
    "- Shape: (1280, {TOP_N_AROUSAL})\n",
    "- Selection: Top {TOP_N_AROUSAL} features ranked by aggregated importance for arousal prediction\n",
    "\n",
    "### 3. Combined Dataset (Valence + Arousal)\n",
    "- File: dataset_combined_top{TOP_N_COMBINED}.csv / X_combined_top{TOP_N_COMBINED}.npy\n",
    "- Features: {TOP_N_COMBINED}\n",
    "- Shape: (1280, {TOP_N_COMBINED})\n",
    "- Selection: Top {TOP_N_COMBINED} features ranked by average importance for both valence and arousal\n",
    "\n",
    "## Target Variables\n",
    "\n",
    "All datasets include 4 target variables:\n",
    "1. **valence_binary**: Binary valence labels (0/1, threshold 4.5)\n",
    "2. **valence_continuous**: Continuous valence ratings (1-9 scale)\n",
    "3. **arousal_binary**: Binary arousal labels (0/1, threshold 4.5)\n",
    "4. **arousal_continuous**: Continuous arousal ratings (1-9 scale)\n",
    "\n",
    "## Feature Importance Methodology\n",
    "\n",
    "Features were ranked using 5 different methods:\n",
    "1. Pearson/Spearman correlation\n",
    "2. ANOVA F-test & Mutual Information\n",
    "3. Random Forest feature importance\n",
    "4. Permutation importance\n",
    "5. Linear SVM coefficients\n",
    "\n",
    "All scores were normalized and averaged to create final rankings.\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Option 1: Load NumPy arrays\n",
    "X = np.load('filtered_features/X_valence_top{TOP_N_VALENCE}.npy')\n",
    "y = np.load('filtered_features/y_valence_binary.npy')\n",
    "\n",
    "# Option 2: Load CSV with feature names\n",
    "df = pd.read_csv('filtered_features/dataset_valence_top{TOP_N_VALENCE}.csv')\n",
    "X = df.drop(['valence_binary', 'valence_continuous', 'arousal_binary', 'arousal_continuous'], axis=1)\n",
    "y = df['valence_binary']\n",
    "```\n",
    "\n",
    "## Files\n",
    "\n",
    "### NumPy Arrays (.npy)\n",
    "- X_valence_top{TOP_N_VALENCE}.npy\n",
    "- X_arousal_top{TOP_N_AROUSAL}.npy\n",
    "- X_combined_top{TOP_N_COMBINED}.npy\n",
    "- y_valence_binary.npy\n",
    "- y_arousal_binary.npy\n",
    "- y_valence_continuous.npy\n",
    "- y_arousal_continuous.npy\n",
    "\n",
    "### CSV Files\n",
    "- dataset_valence_top{TOP_N_VALENCE}.csv\n",
    "- dataset_arousal_top{TOP_N_AROUSAL}.csv\n",
    "- dataset_combined_top{TOP_N_COMBINED}.csv\n",
    "\n",
    "### Feature Lists (.txt)\n",
    "- features_valence_top{TOP_N_VALENCE}.txt\n",
    "- features_arousal_top{TOP_N_AROUSAL}.txt\n",
    "- features_combined_top{TOP_N_COMBINED}.txt\n",
    "\"\"\"\n",
    "\n",
    "with open(output_dir / 'README.md', 'w') as f:\n",
    "    f.write(metadata)\n",
    "\n",
    "print(\"\\nMetadata file created: README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                         FILTERED DATASET SUMMARY\n",
      "================================================================================\n",
      "\n",
      "### Dataset Dimensions ###\n",
      "Original: (1280, 96)\n",
      "Valence-optimized: (1280, 30) - 31.2% of features\n",
      "Arousal-optimized: (1280, 30) - 31.2% of features\n",
      "Combined: (1280, 40) - 41.7% of features\n",
      "\n",
      "### Feature Overlap ###\n",
      "Features in both valence and arousal top-30: 10\n",
      "Overlap percentage: 33.3%\n",
      "\n",
      "Overlapping features:\n",
      "  - AF4_beta\n",
      "  - C4_alpha\n",
      "  - CP1_alpha\n",
      "  - FC1_gamma\n",
      "  - FC2_gamma\n",
      "  - FC6_alpha\n",
      "  - Fp1_beta\n",
      "  - Fp1_gamma\n",
      "  - P4_gamma\n",
      "  - T7_gamma\n",
      "\n",
      "### Target Distribution ###\n",
      "Valence: [472 808] (0: 472, 1: 808)\n",
      "Arousal: [464 816] (0: 464, 1: 816)\n",
      "\n",
      "### Files Created ###\n",
      "Location: ../datasets/DEAP/filtered_features\n",
      "Total files: 14\n",
      "\n",
      "================================================================================\n",
      "\n",
      "✓ Filtered datasets created successfully!\n",
      "\n",
      "You can now use these datasets for training more efficient models.\n",
      "The reduced feature sets should improve training speed and may reduce overfitting.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \"*25 + \"FILTERED DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n### Dataset Dimensions ###\")\n",
    "print(f\"Original: (1280, 96)\")\n",
    "print(f\"Valence-optimized: (1280, {TOP_N_VALENCE}) - {(TOP_N_VALENCE/96)*100:.1f}% of features\")\n",
    "print(f\"Arousal-optimized: (1280, {TOP_N_AROUSAL}) - {(TOP_N_AROUSAL/96)*100:.1f}% of features\")\n",
    "print(f\"Combined: (1280, {TOP_N_COMBINED}) - {(TOP_N_COMBINED/96)*100:.1f}% of features\")\n",
    "\n",
    "print(\"\\n### Feature Overlap ###\")\n",
    "overlap = set(top_features_valence) & set(top_features_arousal)\n",
    "print(f\"Features in both valence and arousal top-{TOP_N_VALENCE}: {len(overlap)}\")\n",
    "print(f\"Overlap percentage: {(len(overlap)/TOP_N_VALENCE)*100:.1f}%\")\n",
    "\n",
    "if len(overlap) > 0:\n",
    "    print(f\"\\nOverlapping features:\")\n",
    "    for feat in sorted(overlap):\n",
    "        print(f\"  - {feat}\")\n",
    "\n",
    "print(\"\\n### Target Distribution ###\")\n",
    "print(f\"Valence: {np.bincount(y_valence)} (0: {np.bincount(y_valence)[0]}, 1: {np.bincount(y_valence)[1]})\")\n",
    "print(f\"Arousal: {np.bincount(y_arousal)} (0: {np.bincount(y_arousal)[0]}, 1: {np.bincount(y_arousal)[1]})\")\n",
    "\n",
    "print(\"\\n### Files Created ###\")\n",
    "print(f\"Location: {output_dir}\")\n",
    "print(f\"Total files: {len(list(output_dir.glob('*')))}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\n✓ Filtered datasets created successfully!\")\n",
    "print(\"\\nYou can now use these datasets for training more efficient models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
