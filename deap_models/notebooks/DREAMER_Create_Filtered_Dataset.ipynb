{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Filtered Dataset with Top Important Features - DREAMER\n",
    "\n",
    "This notebook creates filtered datasets containing only the most important features based on the DREAMER feature importance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "TOP_N_VALENCE = 30  # Number of top features to use for valence prediction\n",
    "TOP_N_AROUSAL = 30  # Number of top features to use for arousal prediction\n",
    "TOP_N_COMBINED = 40  # Number of top features when combining valence & arousal\n",
    "BINARIZE_THRESHOLD = 3.0  # Threshold for binarizing valence/arousal (1-5 scale)\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - Top {TOP_N_VALENCE} features for valence\")\n",
    "print(f\"  - Top {TOP_N_AROUSAL} features for arousal\")\n",
    "print(f\"  - Top {TOP_N_COMBINED} features for combined (valence + arousal)\")\n",
    "print(f\"  - Binarization threshold: {BINARIZE_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Feature Importance Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature importance rankings\n",
    "importance_valence = pd.read_csv('dreamer_feature_importance_valence.csv')\n",
    "importance_arousal = pd.read_csv('dreamer_feature_importance_arousal.csv')\n",
    "\n",
    "print(f\"Loaded feature importance for {len(importance_valence)} features\")\n",
    "print(f\"\\nTop 10 features for VALENCE:\")\n",
    "print(importance_valence[['Feature', 'Valence_AvgScore']].head(10))\n",
    "\n",
    "print(f\"\\nTop 10 features for AROUSAL:\")\n",
    "print(importance_arousal[['Feature', 'Arousal_AvgScore']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Separate features and targets\nmetadata_cols = ['Unnamed: 0', 'patient_index', 'video_index']\ntarget_cols = ['arousal', 'valence']\n\n# Get feature columns\nfeature_cols = [col for col in merged_df.columns if col not in metadata_cols + target_cols]\n\nX = merged_df[feature_cols].values\nfeature_names = feature_cols\n\n# Get targets\ny_valence_cont = merged_df['valence'].values\ny_arousal_cont = merged_df['arousal'].values\n\n# Binarize\ny_valence = (y_valence_cont > BINARIZE_THRESHOLD).astype(int)\ny_arousal = (y_arousal_cont > BINARIZE_THRESHOLD).astype(int)\n\nprint(f\"\\nFeature matrix shape: {X.shape}\")\nprint(f\"Total features: {len(feature_names)}\")\n\n# Clean data (remove features with >50% invalid, impute rest)\nprint(\"\\n=== Data Cleaning ===\")\nnan_counts = np.isnan(X).sum(axis=0)\ninf_counts = np.isinf(X).sum(axis=0)\ninvalid_ratio = (nan_counts + inf_counts) / X.shape[0]\n\nfeatures_to_keep = invalid_ratio <= 0.5\nprint(f\"Removing {(~features_to_keep).sum()} features with >50% invalid values\")\n\nX = X[:, features_to_keep]\nfeature_names = [f for i, f in enumerate(feature_names) if features_to_keep[i]]\n\n# Replace inf with NaN, then impute\nfrom sklearn.impute import SimpleImputer\nX = np.where(np.isinf(X), np.nan, X)\nimputer = SimpleImputer(strategy='median')\nX = imputer.fit_transform(X)\n\nprint(f\"Clean feature matrix shape: {X.shape}\")\nprint(f\"Clean features: {len(feature_names)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Filtered Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top N features for valence\n",
    "top_features_valence = importance_valence['Feature'].head(TOP_N_VALENCE).tolist()\n",
    "\n",
    "# Get indices of these features\n",
    "valence_feature_indices = [feature_names.index(f) for f in top_features_valence]\n",
    "\n",
    "# Create filtered dataset\n",
    "X_valence = X[:, valence_feature_indices]\n",
    "\n",
    "print(f\"\\n=== VALENCE FILTERED DATASET ===\")\n",
    "print(f\"Shape: {X_valence.shape}\")\n",
    "print(f\"Selected features ({len(top_features_valence)}):\")\n",
    "for i, feat in enumerate(top_features_valence, 1):\n",
    "    score = importance_valence[importance_valence['Feature'] == feat]['Valence_AvgScore'].values[0]\n",
    "    print(f\"  {i:2d}. {feat:50s} (score: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top N features for arousal\n",
    "top_features_arousal = importance_arousal['Feature'].head(TOP_N_AROUSAL).tolist()\n",
    "\n",
    "# Get indices of these features\n",
    "arousal_feature_indices = [feature_names.index(f) for f in top_features_arousal]\n",
    "\n",
    "# Create filtered dataset\n",
    "X_arousal = X[:, arousal_feature_indices]\n",
    "\n",
    "print(f\"\\n=== AROUSAL FILTERED DATASET ===\")\n",
    "print(f\"Shape: {X_arousal.shape}\")\n",
    "print(f\"Selected features ({len(top_features_arousal)}):\")\n",
    "for i, feat in enumerate(top_features_arousal, 1):\n",
    "    score = importance_arousal[importance_arousal['Feature'] == feat]['Arousal_AvgScore'].values[0]\n",
    "    print(f\"  {i:2d}. {feat:50s} (score: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined dataset (rank by average of both scores)\n",
    "all_importance = importance_valence[['Feature', 'Valence_AvgScore']].merge(\n",
    "    importance_arousal[['Feature', 'Arousal_AvgScore']], on='Feature'\n",
    ")\n",
    "all_importance['Combined_Score'] = (all_importance['Valence_AvgScore'] + all_importance['Arousal_AvgScore']) / 2\n",
    "all_importance = all_importance.sort_values('Combined_Score', ascending=False)\n",
    "\n",
    "top_features_combined = all_importance['Feature'].head(TOP_N_COMBINED).tolist()\n",
    "combined_feature_indices = [feature_names.index(f) for f in top_features_combined]\n",
    "\n",
    "X_combined = X[:, combined_feature_indices]\n",
    "\n",
    "print(f\"\\n=== COMBINED (VALENCE + AROUSAL) FILTERED DATASET ===\")\n",
    "print(f\"Shape: {X_combined.shape}\")\n",
    "print(f\"Selected features ({len(top_features_combined)}):\")\n",
    "for i, feat in enumerate(top_features_combined, 1):\n",
    "    score = all_importance[all_importance['Feature'] == feat]['Combined_Score'].values[0]\n",
    "    print(f\"  {i:2d}. {feat:50s} (combined score: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Filtered Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('../datasets/Dreamer/filtered_features')\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save as NumPy arrays (.npy)\n",
    "np.save(output_dir / f'X_valence_top{TOP_N_VALENCE}.npy', X_valence)\n",
    "np.save(output_dir / f'X_arousal_top{TOP_N_AROUSAL}.npy', X_arousal)\n",
    "np.save(output_dir / f'X_combined_top{TOP_N_COMBINED}.npy', X_combined)\n",
    "np.save(output_dir / 'y_valence_binary.npy', y_valence)\n",
    "np.save(output_dir / 'y_arousal_binary.npy', y_arousal)\n",
    "np.save(output_dir / 'y_valence_continuous.npy', y_valence_cont)\n",
    "np.save(output_dir / 'y_arousal_continuous.npy', y_arousal_cont)\n",
    "\n",
    "# Save feature names as text files\n",
    "with open(output_dir / f'features_valence_top{TOP_N_VALENCE}.txt', 'w') as f:\n",
    "    f.write('\\n'.join(top_features_valence))\n",
    "\n",
    "with open(output_dir / f'features_arousal_top{TOP_N_AROUSAL}.txt', 'w') as f:\n",
    "    f.write('\\n'.join(top_features_arousal))\n",
    "\n",
    "with open(output_dir / f'features_combined_top{TOP_N_COMBINED}.txt', 'w') as f:\n",
    "    f.write('\\n'.join(top_features_combined))\n",
    "\n",
    "print(\"\\n=== Files Saved ===\")\n",
    "print(f\"Directory: {output_dir}\")\n",
    "print(\"\\nData files:\")\n",
    "print(f\"  - X_valence_top{TOP_N_VALENCE}.npy\")\n",
    "print(f\"  - X_arousal_top{TOP_N_AROUSAL}.npy\")\n",
    "print(f\"  - X_combined_top{TOP_N_COMBINED}.npy\")\n",
    "print(\"  - y_valence_binary.npy\")\n",
    "print(\"  - y_arousal_binary.npy\")\n",
    "print(\"  - y_valence_continuous.npy\")\n",
    "print(\"  - y_arousal_continuous.npy\")\n",
    "print(\"\\nFeature lists:\")\n",
    "print(f\"  - features_valence_top{TOP_N_VALENCE}.txt\")\n",
    "print(f\"  - features_arousal_top{TOP_N_AROUSAL}.txt\")\n",
    "print(f\"  - features_combined_top{TOP_N_COMBINED}.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save as Pandas DataFrames (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames with feature names\n",
    "df_valence = pd.DataFrame(X_valence, columns=top_features_valence)\n",
    "df_valence['valence_binary'] = y_valence\n",
    "df_valence['valence_continuous'] = y_valence_cont\n",
    "df_valence['arousal_binary'] = y_arousal\n",
    "df_valence['arousal_continuous'] = y_arousal_cont\n",
    "\n",
    "df_arousal = pd.DataFrame(X_arousal, columns=top_features_arousal)\n",
    "df_arousal['valence_binary'] = y_valence\n",
    "df_arousal['valence_continuous'] = y_valence_cont\n",
    "df_arousal['arousal_binary'] = y_arousal\n",
    "df_arousal['arousal_continuous'] = y_arousal_cont\n",
    "\n",
    "df_combined = pd.DataFrame(X_combined, columns=top_features_combined)\n",
    "df_combined['valence_binary'] = y_valence\n",
    "df_combined['valence_continuous'] = y_valence_cont\n",
    "df_combined['arousal_binary'] = y_arousal\n",
    "df_combined['arousal_continuous'] = y_arousal_cont\n",
    "\n",
    "# Save as CSV\n",
    "df_valence.to_csv(output_dir / f'dataset_valence_top{TOP_N_VALENCE}.csv', index=False)\n",
    "df_arousal.to_csv(output_dir / f'dataset_arousal_top{TOP_N_AROUSAL}.csv', index=False)\n",
    "df_combined.to_csv(output_dir / f'dataset_combined_top{TOP_N_COMBINED}.csv', index=False)\n",
    "\n",
    "print(\"\\nCSV files saved:\")\n",
    "print(f\"  - dataset_valence_top{TOP_N_VALENCE}.csv\")\n",
    "print(f\"  - dataset_arousal_top{TOP_N_AROUSAL}.csv\")\n",
    "print(f\"  - dataset_combined_top{TOP_N_COMBINED}.csv\")\n",
    "\n",
    "print(f\"\\nDataFrame shapes:\")\n",
    "print(f\"  - Valence: {df_valence.shape}\")\n",
    "print(f\"  - Arousal: {df_arousal.shape}\")\n",
    "print(f\"  - Combined: {df_combined.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Metadata File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a metadata file documenting the filtered datasets\n",
    "metadata = f\"\"\"# DREAMER Filtered Feature Datasets - Metadata\n",
    "\n",
    "Generated from Feature Importance Analysis\n",
    "Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Dataset Information\n",
    "\n",
    "Original dataset: DREAMER (Database for Emotion Recognition using EEG, ECG and Multimodal Signals)\n",
    "- Total samples: {X.shape[0]}\n",
    "- Original features: {X.shape[1]} (combined from features_table.csv and features_table_imf.csv)\n",
    "- EEG channels: 14 (AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8, AF4)\n",
    "- Frequency bands: 6 (delta, theta, alpha, betaL, betaH, gamma)\n",
    "- Feature types: bandpower, activity, mobility, complexity, entropy, wenergy, IMF energy, IMF entropy\n",
    "\n",
    "## Filtered Datasets\n",
    "\n",
    "### 1. Valence-Optimized Dataset\n",
    "- File: dataset_valence_top{TOP_N_VALENCE}.csv / X_valence_top{TOP_N_VALENCE}.npy\n",
    "- Features: {TOP_N_VALENCE}\n",
    "- Shape: ({X.shape[0]}, {TOP_N_VALENCE})\n",
    "- Selection: Top {TOP_N_VALENCE} features ranked by aggregated importance for valence prediction\n",
    "\n",
    "### 2. Arousal-Optimized Dataset\n",
    "- File: dataset_arousal_top{TOP_N_AROUSAL}.csv / X_arousal_top{TOP_N_AROUSAL}.npy\n",
    "- Features: {TOP_N_AROUSAL}\n",
    "- Shape: ({X.shape[0]}, {TOP_N_AROUSAL})\n",
    "- Selection: Top {TOP_N_AROUSAL} features ranked by aggregated importance for arousal prediction\n",
    "\n",
    "### 3. Combined Dataset (Valence + Arousal)\n",
    "- File: dataset_combined_top{TOP_N_COMBINED}.csv / X_combined_top{TOP_N_COMBINED}.npy\n",
    "- Features: {TOP_N_COMBINED}\n",
    "- Shape: ({X.shape[0]}, {TOP_N_COMBINED})\n",
    "- Selection: Top {TOP_N_COMBINED} features ranked by average importance for both valence and arousal\n",
    "\n",
    "## Target Variables\n",
    "\n",
    "All datasets include 4 target variables:\n",
    "1. **valence_binary**: Binary valence labels (0/1, threshold {BINARIZE_THRESHOLD})\n",
    "2. **valence_continuous**: Continuous valence ratings (1-5 scale)\n",
    "3. **arousal_binary**: Binary arousal labels (0/1, threshold {BINARIZE_THRESHOLD})\n",
    "4. **arousal_continuous**: Continuous arousal ratings (1-5 scale)\n",
    "\n",
    "## Feature Importance Methodology\n",
    "\n",
    "Features were ranked using 5 different methods:\n",
    "1. Pearson/Spearman correlation\n",
    "2. ANOVA F-test & Mutual Information\n",
    "3. Random Forest feature importance\n",
    "4. Permutation importance\n",
    "5. Linear SVM coefficients\n",
    "\n",
    "All scores were normalized and averaged to create final rankings.\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Option 1: Load NumPy arrays\n",
    "X = np.load('filtered_features/X_valence_top{TOP_N_VALENCE}.npy')\n",
    "y = np.load('filtered_features/y_valence_binary.npy')\n",
    "\n",
    "# Option 2: Load CSV with feature names\n",
    "df = pd.read_csv('filtered_features/dataset_valence_top{TOP_N_VALENCE}.csv')\n",
    "X = df.drop(['valence_binary', 'valence_continuous', 'arousal_binary', 'arousal_continuous'], axis=1)\n",
    "y = df['valence_binary']\n",
    "```\n",
    "\n",
    "## Files\n",
    "\n",
    "### NumPy Arrays (.npy)\n",
    "- X_valence_top{TOP_N_VALENCE}.npy\n",
    "- X_arousal_top{TOP_N_AROUSAL}.npy\n",
    "- X_combined_top{TOP_N_COMBINED}.npy\n",
    "- y_valence_binary.npy\n",
    "- y_arousal_binary.npy\n",
    "- y_valence_continuous.npy\n",
    "- y_arousal_continuous.npy\n",
    "\n",
    "### CSV Files\n",
    "- dataset_valence_top{TOP_N_VALENCE}.csv\n",
    "- dataset_arousal_top{TOP_N_AROUSAL}.csv\n",
    "- dataset_combined_top{TOP_N_COMBINED}.csv\n",
    "\n",
    "### Feature Lists (.txt)\n",
    "- features_valence_top{TOP_N_VALENCE}.txt\n",
    "- features_arousal_top{TOP_N_AROUSAL}.txt\n",
    "- features_combined_top{TOP_N_COMBINED}.txt\n",
    "\"\"\"\n",
    "\n",
    "with open(output_dir / 'README.md', 'w') as f:\n",
    "    f.write(metadata)\n",
    "\n",
    "print(\"\\nMetadata file created: README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \"*25 + \"FILTERED DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n### Dataset Dimensions ###\")\n",
    "print(f\"Original: ({X.shape[0]}, {X.shape[1]})\")\n",
    "print(f\"Valence-optimized: ({X.shape[0]}, {TOP_N_VALENCE}) - {(TOP_N_VALENCE/X.shape[1])*100:.1f}% of features\")\n",
    "print(f\"Arousal-optimized: ({X.shape[0]}, {TOP_N_AROUSAL}) - {(TOP_N_AROUSAL/X.shape[1])*100:.1f}% of features\")\n",
    "print(f\"Combined: ({X.shape[0]}, {TOP_N_COMBINED}) - {(TOP_N_COMBINED/X.shape[1])*100:.1f}% of features\")\n",
    "\n",
    "print(\"\\n### Feature Overlap ###\")\n",
    "overlap = set(top_features_valence) & set(top_features_arousal)\n",
    "print(f\"Features in both valence and arousal top-{TOP_N_VALENCE}: {len(overlap)}\")\n",
    "print(f\"Overlap percentage: {(len(overlap)/TOP_N_VALENCE)*100:.1f}%\")\n",
    "\n",
    "if len(overlap) > 0 and len(overlap) <= 20:\n",
    "    print(f\"\\nOverlapping features:\")\n",
    "    for feat in sorted(overlap):\n",
    "        print(f\"  - {feat}\")\n",
    "\n",
    "print(\"\\n### Target Distribution ###\")\n",
    "print(f\"Valence: {np.bincount(y_valence)} (0: {np.bincount(y_valence)[0]}, 1: {np.bincount(y_valence)[1]})\")\n",
    "print(f\"Arousal: {np.bincount(y_arousal)} (0: {np.bincount(y_arousal)[0]}, 1: {np.bincount(y_arousal)[1]})\")\n",
    "\n",
    "print(\"\\n### Files Created ###\")\n",
    "print(f\"Location: {output_dir}\")\n",
    "print(f\"Total files: {len(list(output_dir.glob('*')))}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nâœ“ Filtered datasets created successfully!\")\n",
    "print(\"\\nYou can now use these datasets for training more efficient models.\")\n",
    "print(\"The reduced feature sets should improve training speed and may reduce overfitting.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}